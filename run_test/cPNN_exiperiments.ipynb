{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIzcnOlfs90sfskPsM068O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# going to the repository of this note book\n","%cd drive/My Drive/Thesis/CPNN_PiggyBack\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZdukimZf3x4K","executionInfo":{"status":"ok","timestamp":1697364990172,"user_tz":-120,"elapsed":18549,"user":{"displayName":"reza paki","userId":"01979157922216683878"}},"outputId":"8ffeaaa5-6182-4bbd-d8ee-c48f089e0832"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Thesis/CPNN_PiggyBack\n","data\t\t     iris-model.pickle\tperformance\t   requirements.txt  Untitled0.ipynb\n","datasets\t     lab\t\tpush_to_git.ipynb  results\n","iris-model-full.pth  models\t\tREADME.md\t   run_test\n"]}]},{"cell_type":"code","source":["# IMPORT\n","import os\n","from models.cpnn import *\n","from models.cpnn_others import cPNNExp\n","from models.cpnn_seq import cPNNSeq\n","from models.cgru import cGRULinear\n","from models.clstm import *\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import argparse"],"metadata":{"id":"I7eBx3S83u4k","executionInfo":{"status":"ok","timestamp":1697364999960,"user_tz":-120,"elapsed":9816,"user":{"displayName":"reza paki","userId":"01979157922216683878"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["##This part is related to run CPNN with gru layers"],"metadata":{"id":"GX5Ags7UP5I2"}},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmx2jJvj3nv1","executionInfo":{"status":"ok","timestamp":1696855916191,"user_tz":-120,"elapsed":694035,"user":{"displayName":"reza paki","userId":"01979157922216683878"}},"outputId":"a588f138-4d68-4f6b-a6c9-d212023c2603"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'models.cgru.cGRULinear'>\n","sine_rw10_mode5_extended_16-16_2341\n","cGRULinear\n","1/1 iteration of cpnn\n","TASK: 1\n","391 / 391  batch\n","Accuracy media sul task 1: 0.7633751598465474\n","\n","TASK: 2\n","\n","Accuracy media sul task 2: 0.857340952685422\n","\n","TASK: 3\n","\n","Accuracy media sul task 3: 0.826386668797954\n","\n","TASK: 4\n","\n","Accuracy media sul task 4: 0.920488331202046\n","\n","\n"]}],"source":["\n","# EDITABLE PARAMETERS\n","dataset = \"sine_rw10_mode5_extended_16-16_2341\"\n","\n","# OTHER PARAMETERS\n","batch_size = 128\n","hidden_size = 50\n","seq_len = 10\n","# TODO\n","iterations = 1\n","loss_on_seq = False\n","freeze_inputs_weights = False\n","pretraining_samples = 0\n","pretraining_epochs = 0\n","write_weights = False\n","combination = False\n","rembember_initial_states = False\n","suffix = \"\"\n","\n","if freeze_inputs_weights:\n","    suffix += \"_exp\"\n","if combination:\n","    suffix = \"_combination\" + suffix\n","if suffix != \"\" and suffix[0:1] != \"_\":\n","    suffix = \"_\" + suffix\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\n","    \"--model\",\n","    type=str,\n","    default=\"cpnn\",\n","    help=\"Model to use: {'cpnn', 'single': cLSTM, 'multiple': mcLSTM}\",\n",")\n","parser.add_argument(\n","    \"--model_class\",\n","    type=str,\n","    default=\"cgru\",\n","    help=\"Base learner to use: {'clstm', 'cgru'}\",\n",")\n","args,_ = parser.parse_known_args()\n","\n","if args.model_class == \"clstm\":\n","    model_class = cLSTMLinear\n","else:\n","    model_class = cGRULinear\n","if hidden_size is None:\n","    if args.model_class == \"clstm\":\n","        hidden_size = 50\n","    else:\n","        hidden_size = 128\n","device = torch.device(\"cpu\")\n","df = pd.read_csv(os.path.join(\"datasets/datasets\", f\"{dataset}.csv\"))\n","perf_test = {\"accuracy\": [], \"kappa\": [], \"kappa_temporal\": [], \"loss\": []}\n","perf_train = {\"accuracy\": [], \"kappa\": [], \"kappa_temporal\": [], \"loss\": []}\n","perf_anytime = {\"accuracy\": [], \"kappa\": [], \"kappa_temporal\": []}\n","seq_str = \"_seq\" if loss_on_seq else \"\"\n","\n","path = os.path.join(\n","    \"performance\",\n","    f\"{dataset}/{args.model}_{args.model_class}{seq_str}{suffix}_{hidden_size}hs\",\n",")\n","if not os.path.isdir(path):\n","    os.makedirs(path)\n","\n","path_anytime = path + \"_anytime\"\n","if not os.path.isdir(path_anytime):\n","    os.makedirs(path_anytime)\n","\n","# UTILS\n","print(model_class)\n","def create_cpnn():\n","    if not loss_on_seq:\n","        if not freeze_inputs_weights:\n","            return cPNN(column_class=model_class, device=device, seq_len=seq_len, train_verbose=False,\n","                        combination=combination, input_size=len(df.columns) - 2, hidden_size=hidden_size, output_size=2,\n","                        batch_size=batch_size)\n","        else:\n","            return cPNNExp(\n","                column_class=model_class,\n","                input_size=len(df.columns) - 2,\n","                hidden_size=hidden_size,\n","                output_size=2,\n","                batch_size=batch_size,\n","                device=device,\n","                seq_len=seq_len,\n","                train_verbose=False,\n","                combination=combination,\n","                remember_initial_states=rembember_initial_states,\n","            )\n","    return cPNNSeq(\n","        column_class=model_class,\n","        input_size=len(df.columns) - 2,\n","        hidden_size=hidden_size,\n","        output_size=2,\n","        batch_size=batch_size,\n","        device=device,\n","        seq_len=seq_len,\n","    )\n","\n","\n","# MAIN\n","if __name__ == \"__main__\":\n","    if args.model == \"cpnn\" and write_weights:\n","        try:\n","            df_test = pd.read_csv(os.path.join(\"datasets/datasets\", f\"{dataset}_test.csv\"))\n","        except:\n","            pass\n","    models = []\n","    params = []\n","    inputs = []\n","    hiddens = []\n","    acc=[]\n","    kappa=[]\n","\n","    print(dataset)\n","    for i in range(1, iterations + 1):\n","        models.append([])\n","        params.append([])\n","        inputs.append([])\n","        hiddens.append([])\n","\n","        for k in perf_test:\n","            perf_test[k].append([])\n","        for k in perf_train:\n","            perf_train[k].append([])\n","        for k in perf_anytime:\n","            perf_anytime[k].append([])\n","        models[-1].append(create_cpnn())\n","        print(type(models[-1][-1].columns.columns[0]).__name__)\n","        print(f\"{i}/{iterations} iteration of {args.model}\")\n","        for task in range(1, df[\"task\"].max() + 1):\n","            acc.append([])\n","            kappa.append([])\n","            params[-1].append([])\n","            inputs[-1].append([])\n","            hiddens[-1].append([])\n","            print(\"TASK:\", task)\n","            if task > 1:\n","                if args.model == \"cpnn\":\n","                    models[-1][-1].add_new_column()\n","                elif args.model == \"multiple\":\n","                    models[-1].append(create_cpnn())\n","                elif args.model == \"single\":\n","                    models[-1].append(pickle.loads(pickle.dumps(models[-1][-1])))\n","            df_task = df[df[\"task\"] == task]\n","            df_task = df_task.drop(columns=\"task\")\n","\n","            if pretraining_samples > 0:\n","                df_pre = df_task.iloc[:pretraining_samples, 0:]\n","                df_task = df_task.iloc[pretraining_samples:, 0:]\n","                perf_pretraining = models[-1][-1].pretraining(\n","                    df_pre.iloc[0:, :-1].values.astype(np.float32),\n","                    list(df_pre.iloc[0:, -1]),\n","                    pretraining_epochs,\n","                )\n","                with open(\n","                    os.path.join(path, \"pretraining.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(perf_pretraining, f)\n","\n","            for k in perf_test:\n","                perf_test[k][-1].append([])\n","            for k in perf_train:\n","                perf_train[k][-1].append([])\n","            for k in perf_anytime:\n","                perf_anytime[k][-1].append([])\n","            if len(df_task) % batch_size == 0:\n","                n_batches = int(len(df_task) / batch_size)\n","            else:\n","                n_batches = int(len(df_task) / batch_size) + 1\n","            for i in range(0, len(df_task), batch_size):\n","                x = df_task.iloc[i : i + batch_size, 0:-1].values.astype(np.float32)\n","                y = list(df_task.iloc[i : i + batch_size, -1])\n","                print(int(i / batch_size) + 1, \"/\", n_batches, \" batch\", end=\"\\r\")\n","                if len(y) >= seq_len:\n","                    batch_perf_test, batch_perf_anytime, batch_perf_train = models[-1][-1].test_then_train(\n","                        x, y\n","                    )\n","                    acc[-1].append([batch_perf_test['accuracy']])\n","                    kappa[-1].append([batch_perf_test['kappa']])\n","                    for k in batch_perf_test:\n","                        perf_test[k][-1][-1].append(batch_perf_test[k])\n","                    for k in batch_perf_anytime:\n","                        perf_anytime[k][-1][-1].append(batch_perf_anytime[k])\n","                    for k in batch_perf_train:\n","                        perf_train[k][-1][-1].append(batch_perf_train[k])\n","                    if args.model == \"cpnn\" and write_weights:\n","                        try:\n","                            df_test_task = df_test[df_test[\"task\"] == task].drop(\n","                                columns=\"task\"\n","                            )\n","                            x_test = df_test_task.iloc[0:500, 0:-1].values.astype(\n","                                np.float32\n","                            )\n","                            inputs[-1][-1].append(\n","                                models[-1][-1]\n","                                .columns._convert_to_tensor_dataset(x_test)\n","                                .detach()\n","                                .numpy()\n","                            )\n","                            hiddens[-1][-1].append(models[-1][-1].get_hidden(x_test))\n","                        except:\n","                            pass\n","                        params[-1][-1].append(\n","                            pickle.loads(\n","                                pickle.dumps(\n","                                    models[-1][-1]\n","                                    .columns.columns[-1]\n","                                    .lstm.weight_ih_l0.data.detach()\n","                                    .numpy()\n","                                )\n","                            )\n","                        )\n","            print()\n","            print(\n","                f\"Accuracy media sul task {task}: {np.mean(perf_test['accuracy'][-1][-1])}\"\n","            )\n","            print()\n","\n","            with open(\n","                os.path.join(path, \"test_then_train.pkl\"),\n","                \"wb\",\n","            ) as f:\n","                pickle.dump(perf_test, f)\n","\n","            with open(\n","                os.path.join(path_anytime, \"test_then_train.pkl\"),\n","                \"wb\",\n","            ) as f:\n","                pickle.dump(perf_anytime, f)\n","\n","            with open(\n","                os.path.join(path, \"train.pkl\"),\n","                \"wb\",\n","            ) as f:\n","                pickle.dump(perf_train, f)\n","            with open(os.path.join(path, \"models.pkl\"), \"wb\") as f:\n","                pickle.dump(models, f)\n","\n","            if args.model == \"cpnn\" and write_weights:\n","                with open(\n","                    os.path.join(path, \"cpnn_params.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(params, f)\n","\n","                with open(\n","                    os.path.join(path, \"inputs.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(inputs, f)\n","\n","                with open(\n","                    os.path.join(path, \"hiddens.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(hiddens, f)\n","        print()"]},{"cell_type":"code","source":["pd.DataFrame(acc).to_csv('results/acc-cpnn-16_16-2341.csv', index=False)\n","pd.DataFrame(kappa).to_csv('results/kappa-cpnn-16_16-2341.csv', index=False)"],"metadata":{"id":"OGXbc-uC6Cq7","executionInfo":{"status":"ok","timestamp":1696855928687,"user_tz":-120,"elapsed":437,"user":{"displayName":"reza paki","userId":"01979157922216683878"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["##This is for running CPNN with gru and pretrain model based on train data"],"metadata":{"id":"SPeE-nJDQPvt"}},{"cell_type":"code","source":["\n","# EDITABLE PARAMETERS\n","dataset1 = \"sine_rw10_mode5_extended_6-6_1234\"\n","pretrain_data = pd.read_csv(os.path.join(\"datasets/datasets\", f\"{dataset1}.csv\"))\n","pretrain_data = pretrain_data[pretrain_data[\"task\"] == 3]\n","pretrain_data.loc[:, 'task'] = 0\n","dataset2 = \"sine_rw10_mode5_extended_16-16_2143\"\n","\n","# OTHER PARAMETERS\n","batch_size = 128\n","hidden_size = 50\n","seq_len = 10\n","# TODO\n","iterations = 1\n","loss_on_seq = False\n","freeze_inputs_weights = False\n","pretraining_samples = 0\n","pretraining_epochs = 0\n","write_weights = False\n","combination = False\n","rembember_initial_states = False\n","suffix = \"\"\n","\n","if freeze_inputs_weights:\n","    suffix += \"_exp\"\n","if combination:\n","    suffix = \"_combination\" + suffix\n","if suffix != \"\" and suffix[0:1] != \"_\":\n","    suffix = \"_\" + suffix\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\n","    \"--model\",\n","    type=str,\n","    default=\"cpnn\",\n","    help=\"Model to use: {'cpnn', 'single': cLSTM, 'multiple': mcLSTM}\",\n",")\n","parser.add_argument(\n","    \"--model_class\",\n","    type=str,\n","    default=\"cgru\",\n","    help=\"Base learner to use: {'clstm', 'cgru'}\",\n",")\n","args,_ = parser.parse_known_args()\n","\n","if args.model_class == \"clstm\":\n","    model_class = cLSTMLinear\n","else:\n","    model_class = cGRULinear\n","if hidden_size is None:\n","    if args.model_class == \"clstm\":\n","        hidden_size = 50\n","    else:\n","        hidden_size = 128\n","device = torch.device(\"cpu\")\n","df = pd.read_csv(os.path.join(\"datasets/datasets\", f\"{dataset2}.csv\"))\n","\n","df = pretrain_data.append(df, ignore_index=True)\n","perf_test = {\"accuracy\": [], \"kappa\": [], \"kappa_temporal\": [], \"loss\": []}\n","perf_train = {\"accuracy\": [], \"kappa\": [], \"kappa_temporal\": [], \"loss\": []}\n","perf_anytime = {\"accuracy\": [], \"kappa\": [], \"kappa_temporal\": []}\n","seq_str = \"_seq\" if loss_on_seq else \"\"\n","\n","path = os.path.join(\n","    \"performance\",\n","    f\"{dataset2}/{args.model}_{args.model_class}{seq_str}{suffix}_{hidden_size}hs\",\n",")\n","if not os.path.isdir(path):\n","    os.makedirs(path)\n","\n","path_anytime = path + \"_anytime\"\n","if not os.path.isdir(path_anytime):\n","    os.makedirs(path_anytime)\n","\n","# UTILS\n","print(model_class)\n","def create_cpnn():\n","    if not loss_on_seq:\n","        if not freeze_inputs_weights:\n","            return cPNN(column_class=model_class, device=device, seq_len=seq_len, train_verbose=False,\n","                        combination=combination, input_size=len(df.columns) - 2, hidden_size=hidden_size, output_size=2,\n","                        batch_size=batch_size)\n","        else:\n","            return cPNNExp(\n","                column_class=model_class,\n","                input_size=len(df.columns) - 2,\n","                hidden_size=hidden_size,\n","                output_size=2,\n","                batch_size=batch_size,\n","                device=device,\n","                seq_len=seq_len,\n","                train_verbose=False,\n","                combination=combination,\n","                remember_initial_states=rembember_initial_states,\n","            )\n","    return cPNNSeq(\n","        column_class=model_class,\n","        input_size=len(df.columns) - 2,\n","        hidden_size=hidden_size,\n","        output_size=2,\n","        batch_size=batch_size,\n","        device=device,\n","        seq_len=seq_len,\n","    )\n","\n","\n","# MAIN\n","if __name__ == \"__main__\":\n","    if args.model == \"cpnn\" and write_weights:\n","        try:\n","            df_test = pd.read_csv(os.path.join(\"datasets/datasets\", f\"{dataset}_test.csv\"))\n","        except:\n","            pass\n","    models = []\n","    params = []\n","    inputs = []\n","    hiddens = []\n","    acc=[]\n","    kappa=[]\n","\n","    print(dataset2)\n","    for i in range(1, iterations + 1):\n","        models.append([])\n","        params.append([])\n","        inputs.append([])\n","        hiddens.append([])\n","\n","        for k in perf_test:\n","            perf_test[k].append([])\n","        for k in perf_train:\n","            perf_train[k].append([])\n","        for k in perf_anytime:\n","            perf_anytime[k].append([])\n","        models[-1].append(create_cpnn())\n","        print(type(models[-1][-1].columns.columns[0]).__name__)\n","        print(f\"{i}/{iterations} iteration of {args.model}\")\n","        for task in range(0, df[\"task\"].max() + 1):\n","            acc.append([])\n","            kappa.append([])\n","            params[-1].append([])\n","            inputs[-1].append([])\n","            hiddens[-1].append([])\n","            print(\"TASK:\", task)\n","            if task > 0:\n","                if args.model == \"cpnn\":\n","                    models[-1][-1].add_new_column()\n","                elif args.model == \"multiple\":\n","                    models[-1].append(create_cpnn())\n","                elif args.model == \"single\":\n","                    models[-1].append(pickle.loads(pickle.dumps(models[-1][-1])))\n","            df_task = df[df[\"task\"] == task]\n","            #df_task.loc[:, 'task'] = 1\n","            df_task = df_task.drop(columns=\"task\")\n","            if pretraining_samples > 0:\n","                df_pre = df_task.iloc[:pretraining_samples, 0:]\n","                df_task = df_task.iloc[pretraining_samples:, 0:]\n","                perf_pretraining = models[-1][-1].pretraining(\n","                    df_pre.iloc[0:, :-1].values.astype(np.float32),\n","                    list(df_pre.iloc[0:, -1]),\n","                    pretraining_epochs,\n","                )\n","                with open(\n","                    os.path.join(path, \"pretraining.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(perf_pretraining, f)\n","\n","            for k in perf_test:\n","                perf_test[k][-1].append([])\n","            for k in perf_train:\n","                perf_train[k][-1].append([])\n","            for k in perf_anytime:\n","                perf_anytime[k][-1].append([])\n","            if len(df_task) % batch_size == 0:\n","                n_batches = int(len(df_task) / batch_size)\n","            else:\n","                n_batches = int(len(df_task) / batch_size) + 1\n","            for i in range(0, len(df_task), batch_size):\n","                x = df_task.iloc[i : i + batch_size, 0:-1].values.astype(np.float32)\n","                y = list(df_task.iloc[i : i + batch_size, -1])\n","                print(int(i / batch_size) + 1, \"/\", n_batches, \" batch\", end=\"\\r\")\n","                if len(y) >= seq_len:\n","                    batch_perf_test, batch_perf_anytime, batch_perf_train = models[-1][-1].test_then_train(\n","                        x, y\n","                    )\n","                    acc[-1].append([batch_perf_test['accuracy']])\n","                    kappa[-1].append([batch_perf_test['kappa']])\n","                    for k in batch_perf_test:\n","                        perf_test[k][-1][-1].append(batch_perf_test[k])\n","                    for k in batch_perf_anytime:\n","                        perf_anytime[k][-1][-1].append(batch_perf_anytime[k])\n","                    for k in batch_perf_train:\n","                        perf_train[k][-1][-1].append(batch_perf_train[k])\n","                    if args.model == \"cpnn\" and write_weights:\n","                        try:\n","                            df_test_task = df_test[df_test[\"task\"] == task].drop(\n","                                columns=\"task\"\n","                            )\n","                            x_test = df_test_task.iloc[0:500, 0:-1].values.astype(\n","                                np.float32\n","                            )\n","                            inputs[-1][-1].append(\n","                                models[-1][-1]\n","                                .columns._convert_to_tensor_dataset(x_test)\n","                                .detach()\n","                                .numpy()\n","                            )\n","                            hiddens[-1][-1].append(models[-1][-1].get_hidden(x_test))\n","                        except:\n","                            pass\n","                        params[-1][-1].append(\n","                            pickle.loads(\n","                                pickle.dumps(\n","                                    models[-1][-1]\n","                                    .columns.columns[-1]\n","                                    .lstm.weight_ih_l0.data.detach()\n","                                    .numpy()\n","                                )\n","                            )\n","                        )\n","            print()\n","            print(\n","                f\"Accuracy media sul task {task}: {np.mean(perf_test['accuracy'][-1][-1])}\"\n","            )\n","            print()\n","\n","            with open(\n","                os.path.join(path, \"test_then_train.pkl\"),\n","                \"wb\",\n","            ) as f:\n","                pickle.dump(perf_test, f)\n","\n","            with open(\n","                os.path.join(path_anytime, \"test_then_train.pkl\"),\n","                \"wb\",\n","            ) as f:\n","                pickle.dump(perf_anytime, f)\n","\n","            with open(\n","                os.path.join(path, \"train.pkl\"),\n","                \"wb\",\n","            ) as f:\n","                pickle.dump(perf_train, f)\n","            with open(os.path.join(path, \"models.pkl\"), \"wb\") as f:\n","                pickle.dump(models, f)\n","\n","            if args.model == \"cpnn\" and write_weights:\n","                with open(\n","                    os.path.join(path, \"cpnn_params.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(params, f)\n","\n","                with open(\n","                    os.path.join(path, \"inputs.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(inputs, f)\n","\n","                with open(\n","                    os.path.join(path, \"hiddens.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(hiddens, f)\n","        print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRP85JMtI3fq","executionInfo":{"status":"ok","timestamp":1697366004118,"user_tz":-120,"elapsed":945484,"user":{"displayName":"reza paki","userId":"01979157922216683878"}},"outputId":"fb7187b4-0c06-4de8-eaec-dfe16e75937b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-b9d61239bb33>:56: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df = pretrain_data.append(df, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<class 'models.cgru.cGRULinear'>\n","sine_rw10_mode5_extended_16-16_2143\n","cGRULinear\n","1/1 iteration of cpnn\n","TASK: 0\n","391 / 391  batch\n","Accuracy media sul task 0: 0.8490688938618925\n","\n","TASK: 1\n","\n","Accuracy media sul task 1: 0.8157888427109974\n","\n","TASK: 2\n","\n","Accuracy media sul task 2: 0.8422114769820972\n","\n","TASK: 3\n","\n","Accuracy media sul task 3: 0.8468030690537085\n","\n","TASK: 4\n","\n","Accuracy media sul task 4: 0.9016703964194374\n","\n","\n"]}]},{"cell_type":"code","source":["pd.DataFrame(acc).to_csv('results/acc-cpnn-pretrain-t3-16_16-2143.csv', index=False)\n","pd.DataFrame(kappa).to_csv('results/kappa-cpnn-pretrain-t3-16_16-2143.csv', index=False)"],"metadata":{"id":"TetdePx3YRce","executionInfo":{"status":"ok","timestamp":1697366004119,"user_tz":-120,"elapsed":28,"user":{"displayName":"reza paki","userId":"01979157922216683878"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sdYnMno_gRo6"},"execution_count":null,"outputs":[]}]}