{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNch+lgyICywFcVNYzqhgJS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# going to the repository of this note book\n","%cd drive/My Drive/Thesis/CPNN_PiggyBack\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZdukimZf3x4K","executionInfo":{"status":"ok","timestamp":1698222932011,"user_tz":-120,"elapsed":17056,"user":{"displayName":"reza paki","userId":"01979157922216683878"}},"outputId":"e280e519-4004-4d5d-bda8-fae008bacb64"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Thesis/CPNN_PiggyBack\n","data\t\t     iris-model.pickle\tperformance\t   requirements.txt  Untitled0.ipynb\n","datasets\t     lab\t\tpush_to_git.ipynb  results\n","iris-model-full.pth  models\t\tREADME.md\t   run_test\n"]}]},{"cell_type":"code","source":["# IMPORT\n","import os\n","from models.cpnn import *\n","from models.cpnn_others import cPNNExp\n","from models.cpnn_seq import cPNNSeq\n","from models.cgru import cGRULinear\n","from models.clstm import *\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import argparse"],"metadata":{"id":"I7eBx3S83u4k","executionInfo":{"status":"ok","timestamp":1698222945987,"user_tz":-120,"elapsed":13982,"user":{"displayName":"reza paki","userId":"01979157922216683878"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["##This part is related to run CPNN with gru layers"],"metadata":{"id":"GX5Ags7UP5I2"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"kmx2jJvj3nv1","executionInfo":{"status":"error","timestamp":1698223384759,"user_tz":-120,"elapsed":3528,"user":{"displayName":"reza paki","userId":"01979157922216683878"}},"outputId":"a19c58da-24a8-4bc6-fd97-5af012eef063"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'models.cgru.cGRULinear'>\n","sine_rw10_mode5_extended_16-16_2341\n","cGRULinear\n","1/1 iteration of cpnn\n","TASK: 1\n","1 / 391  batch\r(128, 2)\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","(128, 2)\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","(128, 2)\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","(128, 2)\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","(128, 2)\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","(128, 2)\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","(128, 2)\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","(128, 2)\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","(128, 2)\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","torch.Size([119, 10, 2])\n","inside the fit model\n","(128, 2)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-8af1652cbfbe>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                     batch_perf_test, batch_perf_anytime, batch_perf_train = models[-1][-1].test_then_train(\n\u001b[0m\u001b[1;32m    177\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                     )\n","\u001b[0;32m<ipython-input-4-e7d618299f9e>\u001b[0m in \u001b[0;36mtest_then_train\u001b[0;34m(self, x, y, column_id)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mperf_test_single_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_many_anytime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mperf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-e7d618299f9e>\u001b[0m in \u001b[0;36mtest_many_anytime\u001b[0;34m(self, x, y, column_id)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \"\"\"\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-e7d618299f9e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \"\"\"\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-e7d618299f9e>\u001b[0m in \u001b[0;36mpredict_one\u001b[0;34m(self, x, column_id, previous_data_points)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_data_points_anytime_inference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_data_points_anytime_inference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_tensor_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_data_points_anytime_inference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_data_points_anytime_inference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_data_points_anytime_inference\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-e7d618299f9e>\u001b[0m in \u001b[0;36m_convert_to_tensor_dataset\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \"\"\"\n\u001b[1;32m    173\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cut_in_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","# EDITABLE PARAMETERS\n","dataset = \"sine_rw10_mode5_extended_16-16_2341\"\n","\n","# OTHER PARAMETERS\n","batch_size = 128\n","hidden_size = 50\n","seq_len = 10\n","# TODO\n","iterations = 1\n","loss_on_seq = False\n","freeze_inputs_weights = False\n","pretraining_samples = 0\n","pretraining_epochs = 0\n","write_weights = False\n","combination = False\n","rembember_initial_states = False\n","suffix = \"\"\n","\n","if freeze_inputs_weights:\n","    suffix += \"_exp\"\n","if combination:\n","    suffix = \"_combination\" + suffix\n","if suffix != \"\" and suffix[0:1] != \"_\":\n","    suffix = \"_\" + suffix\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\n","    \"--model\",\n","    type=str,\n","    default=\"cpnn\",\n","    help=\"Model to use: {'cpnn', 'single': cLSTM, 'multiple': mcLSTM}\",\n",")\n","parser.add_argument(\n","    \"--model_class\",\n","    type=str,\n","    default=\"cgru\",\n","    help=\"Base learner to use: {'clstm', 'cgru'}\",\n",")\n","args,_ = parser.parse_known_args()\n","\n","if args.model_class == \"clstm\":\n","    model_class = cLSTMLinear\n","else:\n","    model_class = cGRULinear\n","if hidden_size is None:\n","    if args.model_class == \"clstm\":\n","        hidden_size = 50\n","    else:\n","        hidden_size = 128\n","device = torch.device(\"cpu\")\n","df = pd.read_csv(os.path.join(\"datasets/datasets\", f\"{dataset}.csv\"))\n","perf_test = {\"accuracy\": [], \"kappa\": [], \"kappa_temporal\": [], \"loss\": []}\n","perf_train = {\"accuracy\": [], \"kappa\": [], \"kappa_temporal\": [], \"loss\": []}\n","perf_anytime = {\"accuracy\": [], \"kappa\": [], \"kappa_temporal\": []}\n","seq_str = \"_seq\" if loss_on_seq else \"\"\n","\n","path = os.path.join(\n","    \"performance\",\n","    f\"{dataset}/{args.model}_{args.model_class}{seq_str}{suffix}_{hidden_size}hs\",\n",")\n","if not os.path.isdir(path):\n","    os.makedirs(path)\n","\n","path_anytime = path + \"_anytime\"\n","if not os.path.isdir(path_anytime):\n","    os.makedirs(path_anytime)\n","\n","# UTILS\n","print(model_class)\n","def create_cpnn():\n","    if not loss_on_seq:\n","        if not freeze_inputs_weights:\n","            return cPNN(column_class=model_class, device=device, seq_len=seq_len, train_verbose=False,\n","                        combination=combination, input_size=len(df.columns) - 2, hidden_size=hidden_size, output_size=2,\n","                        batch_size=batch_size)\n","        else:\n","            return cPNNExp(\n","                column_class=model_class,\n","                input_size=len(df.columns) - 2,\n","                hidden_size=hidden_size,\n","                output_size=2,\n","                batch_size=batch_size,\n","                device=device,\n","                seq_len=seq_len,\n","                train_verbose=False,\n","                combination=combination,\n","                remember_initial_states=rembember_initial_states,\n","            )\n","    return cPNNSeq(\n","        column_class=model_class,\n","        input_size=len(df.columns) - 2,\n","        hidden_size=hidden_size,\n","        output_size=2,\n","        batch_size=batch_size,\n","        device=device,\n","        seq_len=seq_len,\n","    )\n","\n","\n","# MAIN\n","if __name__ == \"__main__\":\n","    if args.model == \"cpnn\" and write_weights:\n","        try:\n","            df_test = pd.read_csv(os.path.join(\"datasets/datasets\", f\"{dataset}_test.csv\"))\n","        except:\n","            pass\n","    models = []\n","    params = []\n","    inputs = []\n","    hiddens = []\n","    acc=[]\n","    kappa=[]\n","\n","    print(dataset)\n","    for i in range(1, iterations + 1):\n","        models.append([])\n","        params.append([])\n","        inputs.append([])\n","        hiddens.append([])\n","\n","        for k in perf_test:\n","            perf_test[k].append([])\n","        for k in perf_train:\n","            perf_train[k].append([])\n","        for k in perf_anytime:\n","            perf_anytime[k].append([])\n","        models[-1].append(create_cpnn())\n","        print(type(models[-1][-1].columns.columns[0]).__name__)\n","        print(f\"{i}/{iterations} iteration of {args.model}\")\n","        for task in range(1, df[\"task\"].max() + 1):\n","            acc.append([])\n","            kappa.append([])\n","            params[-1].append([])\n","            inputs[-1].append([])\n","            hiddens[-1].append([])\n","            print(\"TASK:\", task)\n","            if task > 1:\n","                if args.model == \"cpnn\":\n","                    models[-1][-1].add_new_column()\n","                elif args.model == \"multiple\":\n","                    models[-1].append(create_cpnn())\n","                elif args.model == \"single\":\n","                    models[-1].append(pickle.loads(pickle.dumps(models[-1][-1])))\n","            df_task = df[df[\"task\"] == task]\n","            df_task = df_task.drop(columns=\"task\")\n","\n","            if pretraining_samples > 0:\n","                df_pre = df_task.iloc[:pretraining_samples, 0:]\n","                df_task = df_task.iloc[pretraining_samples:, 0:]\n","                perf_pretraining = models[-1][-1].pretraining(\n","                    df_pre.iloc[0:, :-1].values.astype(np.float32),\n","                    list(df_pre.iloc[0:, -1]),\n","                    pretraining_epochs,\n","                )\n","                with open(\n","                    os.path.join(path, \"pretraining.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(perf_pretraining, f)\n","\n","            for k in perf_test:\n","                perf_test[k][-1].append([])\n","            for k in perf_train:\n","                perf_train[k][-1].append([])\n","            for k in perf_anytime:\n","                perf_anytime[k][-1].append([])\n","            if len(df_task) % batch_size == 0:\n","                n_batches = int(len(df_task) / batch_size)\n","            else:\n","                n_batches = int(len(df_task) / batch_size) + 1\n","            for i in range(0, len(df_task), batch_size):\n","                x = df_task.iloc[i : i + batch_size, 0:-1].values.astype(np.float32)\n","                y = list(df_task.iloc[i : i + batch_size, -1])\n","                print(int(i / batch_size) + 1, \"/\", n_batches, \" batch\", end=\"\\r\")\n","                print(x.shape)\n","                if len(y) >= seq_len:\n","                    batch_perf_test, batch_perf_anytime, batch_perf_train = models[-1][-1].test_then_train(\n","                        x, y\n","                    )\n","                    acc[-1].append([batch_perf_test['accuracy']])\n","                    kappa[-1].append([batch_perf_test['kappa']])\n","                    for k in batch_perf_test:\n","                        perf_test[k][-1][-1].append(batch_perf_test[k])\n","                    for k in batch_perf_anytime:\n","                        perf_anytime[k][-1][-1].append(batch_perf_anytime[k])\n","                    for k in batch_perf_train:\n","                        perf_train[k][-1][-1].append(batch_perf_train[k])\n","                    if args.model == \"cpnn\" and write_weights:\n","                        try:\n","                            df_test_task = df_test[df_test[\"task\"] == task].drop(\n","                                columns=\"task\"\n","                            )\n","                            x_test = df_test_task.iloc[0:500, 0:-1].values.astype(\n","                                np.float32\n","                            )\n","                            inputs[-1][-1].append(\n","                                models[-1][-1]\n","                                .columns._convert_to_tensor_dataset(x_test)\n","                                .detach()\n","                                .numpy()\n","                            )\n","                            hiddens[-1][-1].append(models[-1][-1].get_hidden(x_test))\n","                        except:\n","                            pass\n","                        params[-1][-1].append(\n","                            pickle.loads(\n","                                pickle.dumps(\n","                                    models[-1][-1]\n","                                    .columns.columns[-1]\n","                                    .lstm.weight_ih_l0.data.detach()\n","                                    .numpy()\n","                                )\n","                            )\n","                        )\n","            print()\n","            print(\n","                f\"Accuracy media sul task {task}: {np.mean(perf_test['accuracy'][-1][-1])}\"\n","            )\n","            print()\n","\n","            with open(\n","                os.path.join(path, \"test_then_train.pkl\"),\n","                \"wb\",\n","            ) as f:\n","                pickle.dump(perf_test, f)\n","\n","            with open(\n","                os.path.join(path_anytime, \"test_then_train.pkl\"),\n","                \"wb\",\n","            ) as f:\n","                pickle.dump(perf_anytime, f)\n","\n","            with open(\n","                os.path.join(path, \"train.pkl\"),\n","                \"wb\",\n","            ) as f:\n","                pickle.dump(perf_train, f)\n","            with open(os.path.join(path, \"models.pkl\"), \"wb\") as f:\n","                pickle.dump(models, f)\n","\n","            if args.model == \"cpnn\" and write_weights:\n","                with open(\n","                    os.path.join(path, \"cpnn_params.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(params, f)\n","\n","                with open(\n","                    os.path.join(path, \"inputs.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(inputs, f)\n","\n","                with open(\n","                    os.path.join(path, \"hiddens.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(hiddens, f)\n","        print()"]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from sklearn.metrics import accuracy_score, cohen_kappa_score\n","import warnings\n","\n","from models.cpnn_columns import cPNNColumns\n","from models.utils import (\n","    customized_loss,\n","    accuracy,\n","    cohen_kappa,\n","    kappa_temporal,\n","    get_samples_outputs,\n","    get_pred_from_outputs, kappa_temporal_score,\n",")\n","import torch.utils.data as data_utils\n","from torch.utils.data import DataLoader\n","from models.clstm import (\n","    cLSTMLinear,\n",")\n","from models.cgru import (\n","    cGRULinear,\n",")\n","\n","class cPNN:\n","    \"\"\"\n","    Class that implements all the cPNN structure.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        column_class=cGRULinear,\n","        device=None,\n","        lr: float = 0.01,\n","        seq_len: int = 5,\n","        stride: int = 1,\n","        first_label_kappa: int = None,\n","        train_epochs: int = 5,\n","        train_verbose: bool = False,\n","        concepts_boundaries: list = None,\n","        combination: bool = False,\n","        anytime_learner: bool = False,\n","        loss_on_seq: bool = False,\n","        remember_states: bool = False,\n","        quantize: bool = False,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        column_class: default: cLSTMLinear.\n","            The class that implements the column.\n","        device: default: None.\n","            Torch's device, if None its value is set to 'cpu'.\n","        lr: float, default: 0.01.\n","            The learning rate value of single columns' Adam Optimizer.\n","        seq_len: int, default: 5.\n","            The length of the sliding window that builds the single sequences.\n","        stride: int, default: 1.\n","            The length of the sliding window's stride.\n","        first_label_kappa: int, default: None.\n","            The label of the last sample before the start of the stream, it is used to compute the kappa_temporal.\n","            If None a random label is generated.\n","        train_epochs: int, default: 10.\n","            In case of anytime_learner=False, the training epochs to perform in learn_many method.\n","        train_verbose: bool, default:False.\n","            True if, during the learn_many execution, you want to print the metrics after each training epoch.\n","        concepts_boundaries: list, default:None.\n","            If not None it represents the boundaries of each concept (its last sample's index).\n","            It is used to automatically add a new column after a concept drift.\n","        combination: bool, default: False.\n","            If True each cPNN column combines all previous columns.\n","            If False each cPNN column takes only last column.\n","        anytime_learner: bool, default: False.\n","            If True the model learns data point by data point by data point.\n","            Otherwise, il learns batch by batch.\n","        loss_on_seq: bool, default: False.\n","            In case of anytime_learner = False, if True the model considers only past temporal dependencies. The model\n","            is a many_to_one model and each data point's prediction is associated to the first sequence in which\n","            it appears.\n","            If False, the model considers both past and future temporal dependencies.The model is a many_to_many model\n","            and each data point's prediction is the average prediction between all the sequences in which it appears.\n","        remember_states: bool, default: False\n","            In case of anytime learner and cGRU, if True the initial h0 is set as h1 of the previous sequence.\n","        quantize: bool, default: False\n","            If True, after a concept drift, the column is quantized.\n","        kwargs:\n","            Parameters of column_class.\n","        \"\"\"\n","        self.anytime_learner = anytime_learner\n","        if self.anytime_learner:\n","            self.loss_on_seq = True\n","            self.many_to_one = True\n","            self.remember_states = remember_states\n","        else:\n","            self.loss_on_seq = loss_on_seq\n","            if loss_on_seq:\n","                self.many_to_one = True\n","            else:\n","                self.many_to_one = False\n","            self.remember_states = False\n","\n","        self.columns_args = kwargs\n","        self.columns_args[\"column_class\"] = column_class\n","        self.columns_args[\"device\"] = device\n","        self.columns_args[\"lr\"] = lr\n","        self.columns_args[\"combination\"] = combination\n","        self.columns_args[\"remember_states\"] = self.remember_states\n","        self.columns_args[\"many_to_one\"] = self.many_to_one\n","        self.columns_args[\"quantize\"] = quantize\n","        self.columns = cPNNColumns(**self.columns_args)\n","        self.seq_len = seq_len\n","        self.stride = stride\n","        self.train_epochs = train_epochs\n","        self.train_verbose = train_verbose\n","        self.concept_boundaries = concepts_boundaries\n","        self.samples_cont = 0\n","        self.previous_data_points_anytime_inference = None\n","        self.previous_data_points_anytime_train = None\n","        self.previous_data_points_batch_train = None\n","        self.previous_data_points_batch_test = None\n","\n","        if first_label_kappa is not None:\n","            self.first_label_kappa = torch.tensor([first_label_kappa]).view(1)\n","        else:\n","            self.first_label_kappa = torch.randint(0, 2, (1,)).view(1)\n","\n","    def get_seq_len(self):\n","        return self.seq_len\n","\n","    def _cut_in_sequences(self, x, y):\n","        seqs_features = []\n","        seqs_targets = []\n","        for i in range(0, len(x), self.stride):\n","            if len(x) - i >= self.seq_len:\n","                seqs_features.append(x[i : i + self.seq_len, :].astype(np.float32))\n","                if y is not None:\n","                    seqs_targets.append(\n","                        np.asarray(y[i : i + self.seq_len], dtype=np.int_)\n","                    )\n","        return np.asarray(seqs_features), np.asarray(seqs_targets)\n","\n","    def _cut_in_sequences_tensors(self, x, y):\n","        seqs_features = []\n","        seqs_targets = []\n","        for i in range(0, x.size()[0], self.stride):\n","            if x.size()[0] - i >= self.seq_len:\n","                seqs_features.append(\n","                    x[i : i + self.seq_len, :].view(1, self.seq_len, x.size()[1])\n","                )\n","                seqs_targets.append(y[i : i + self.seq_len].view(1, self.seq_len))\n","        seq_features = torch.cat(seqs_features, dim=0)\n","        seqs_targets = torch.cat(seqs_targets, dim=0)\n","        return seq_features, seqs_targets\n","\n","    def _convert_to_tensor_dataset(self, x, y=None):\n","        \"\"\"\n","        It converts the dataset in order to be inputted to cPNN, by building the different sequences and\n","        converting them to TensorDataset.\n","\n","        Parameters\n","        ----------\n","        x: numpy.array\n","            The features values of the batch.\n","        y: list, default: None\n","            The target values of the batch. If None only features will be loaded.\n","        Returns\n","        -------\n","        dataset: torch.data_utils.TensorDataset\n","            The tensor dataset representing the different sequences.\n","            The features values have shape: (batch_size - seq_len + 1, seq_len, n_features)\n","            The target values have shape: (batch_size - seq_len + 1, seq_len)\n","        \"\"\"\n","        x, y = self._cut_in_sequences(x, y)\n","        x = torch.tensor(x)\n","        if len(y) > 0:\n","            y = torch.tensor(y).type(torch.LongTensor)\n","            return data_utils.TensorDataset(x, y)\n","        return x\n","\n","    def _load_batch(self, x: np.array, y: np.array = None):\n","        \"\"\"\n","        It transforms the batch in order to be inputted to cPNN, by building the different sequences and\n","        converting them to tensors.\n","\n","        Parameters\n","        ----------\n","        x: numpy.array\n","            The features values of the batch.\n","        y: list, default: None.\n","            The target values of the batch. If None only features will be loaded.\n","        Returns\n","        -------\n","        x: torch.Tensor\n","            The features values of the created sequences. It has shape: (batch_size - seq_len + 1, seq_len, n_features)\n","        y: torch.Tensor\n","            The target values of the samples in the batc. It has length: batch_size. If y is None it returns None.\n","        y_seq: torch.Tensor\n","            The target values of the created sequences. It has shape: (batch_size - seq_len + 1, seq_len). If y is None it returns None.\n","        \"\"\"\n","        batch = self._convert_to_tensor_dataset(x, y)\n","        batch_loader = DataLoader(\n","            batch, batch_size=batch.tensors[0].size()[0], drop_last=False\n","        )\n","        y_seq = None\n","        for x, y_seq in batch_loader:  # only to take x and y from loader\n","            break\n","        y = torch.tensor(y)\n","        return x, y, y_seq\n","\n","    def add_new_column(self):\n","        \"\"\"\n","        It adds a new column to the cPNN architecture, after a concept drift.\n","        \"\"\"\n","        self.reset_previous_data_points()\n","        self.columns.add_new_column()\n","\n","    def learn_one(self, x:np.array, y: np.array, previous_data_points: np.array = None):\n","        \"\"\"\n","        It trains cPNN on a single data point.\n","        Before performing the training, if concept_boundaries was provided during the constructor method, it\n","        automatically adds a new column after concept drift.\n","        *ONLY FOR ANYTIME LEARNER*\n","\n","        Parameters\n","        ----------\n","        x: numpy.array or list\n","            The features values of the single data point.\n","        y: numpy.array or list\n","            The target value of the single data point.\n","        previous_data_points: numpy.array, default: None.\n","            The features value of the data points preceding x in the sequence.\n","            If None, it uses the last seq_len-1 points seen during the last calls of the method.\n","            It returns None if the model has not seen yet seq_len-1 data points and previous_data_points is None.\n","        \"\"\"\n","        if not self.anytime_learner:\n","            warnings.warn(\n","                \"The model is a batch learner, it cannot learn from a single data point.\\n\" +\n","                \"Call learn_many on a batch containing a single sequence\"\n","            )\n","            return None\n","        if self.concept_boundaries is not None and len(self.concept_boundaries) > 0:\n","            if self.samples_cont >= self.concept_boundaries[0]:\n","                print(\"New column added\")\n","                self.add_new_column()\n","                self.concept_boundaries = self.concept_boundaries[1:]\n","\n","        x = np.array(x).reshape(1, -1)\n","        y = np.array(y).reshape(1, -1)\n","        if previous_data_points is not None:\n","            self.previous_data_points_anytime_train = previous_data_points\n","        if self.previous_data_points_anytime_train is None:\n","            self.previous_data_points_anytime_train = x\n","            return None\n","        if len(self.previous_data_points_anytime_train) != self.seq_len - 1:\n","            self.previous_data_points_anytime_train = np.concatenate(\n","                [self.previous_data_points_anytime_train, x])\n","            return None\n","        self.previous_data_points_anytime_train = np.concatenate([self.previous_data_points_anytime_train, x])\n","        x, y, _ = self._load_batch(self.previous_data_points_anytime_train, y)\n","        self._fit(x, y.view(-1))\n","        self.previous_data_points_anytime_train = self.previous_data_points_anytime_train[1:]\n","\n","    def learn_many(self, x: np.array, y: np.array) -> dict:\n","        \"\"\"\n","        It trains cPNN on a single batch.\n","        It computes the loss after averaging each sample's predictions.\n","        Before performing the training, if concept_boundaries was provided during the constructor method, it\n","        automatically adds a new column after concept drift.\n","        *ONLY FOR BATCH LEARNER*\n","\n","        Parameters\n","        ----------\n","        x: numpy.array or list\n","            The features values of the batch.\n","        y: np.array or list\n","            The target values of the batch.\n","\n","        Returns\n","        -------\n","        perf_train: dict\n","            The dictionary representing training's performance. Each key contains the list representing all the epochs' performances.\n","            The following metrics are computed: accuracy, loss, kappa, kappa_temporal.\n","            For each metric the dict contains a list of epochs' values.\n","        \"\"\"\n","        if self.anytime_learner:\n","            warnings.warn(\n","                \"The model is an anytime learner, it cannot learn from batch.\\n\" +\n","                \"Loop on learn_one method to learn from multiple data points\"\n","            )\n","            return {}\n","        if self.concept_boundaries is not None and len(self.concept_boundaries) > 0:\n","            if self.samples_cont >= self.concept_boundaries[0]:\n","                print(\"New column added\")\n","                self.add_new_column()\n","                self.concept_boundaries = self.concept_boundaries[1:]\n","\n","        x = np.array(x)\n","        y = list(y)\n","        first_batch = False\n","        if self.loss_on_seq:\n","            if self.previous_data_points_batch_train is None:\n","                first_batch = True\n","            else:\n","                x = np.concatenate([x, self.previous_data_points_batch_train], axis=0)\n","                self.previous_data_points_batch_train = x[-(self.seq_len-1):]\n","        x, y, y_seq = self._load_batch(x, y)\n","        if first_batch:\n","            y = y[self.seq_len - 1:]\n","\n","        perf_train = {\n","            \"accuracy\": [],\n","            \"loss\": [],\n","            \"kappa\": [],\n","            \"kappa_temporal\": [],\n","        }\n","        for e in range(1, self.train_epochs + 1):\n","            perf_epoch = self._fit(x, y)\n","            if self.train_verbose:\n","                print(\n","                    \"Training epoch \",\n","                    e,\n","                    \"/\",\n","                    self.train_epochs,\n","                    \". accuracy: \",\n","                    perf_epoch[\"accuracies\"],\n","                    \", loss:\",\n","                    perf_epoch[\"losses\"],\n","                    sep=\"\",\n","                    end=\"\\r\",\n","                )\n","            for k in perf_epoch:\n","                perf_train[k].append(perf_epoch[k])\n","        if self.train_verbose:\n","            print()\n","            print()\n","        self.samples_cont += x.size()[0]\n","\n","        return perf_train\n","\n","    def predict_many(self, x: np.array, column_id: int = None):\n","        \"\"\"\n","        It performs prediction on a single batch. It uses the last column of cPNN architecture.\n","\n","        Parameters\n","        ----------\n","        x: numpy.array or list\n","            The features values of the batch.\n","        column_id: int, default: None.\n","            The id of the column to use. If None the last column is used.\n","\n","        Returns\n","        -------\n","        predictions: numpy.array\n","            The 1D numpy array (with length batch_size) containing predictions of all samples.\n","        \"\"\"\n","\n","        if self.anytime_learner:\n","            if self.anytime_learner:\n","                warnings.warn(\n","                    \"The model is an anytime learner, it cannot predict a batch of data.\\n\" +\n","                    \"Loop on predict_one method to predict on multiple data points\"\n","                )\n","                return None\n","\n","        x = np.array(x)\n","        if x.shape[0] < self.get_seq_len():\n","            return np.array([None] * x.shape[0])\n","        first_train = False\n","\n","        if self.loss_on_seq:\n","            if self.previous_data_points_batch_train is not None:\n","                x = np.concatenate([x, self.previous_data_points_batch_train], axis=0)\n","                self.previous_data_points_batch_train = x[-(self.seq_len-1):]\n","            else:\n","                first_train = True\n","\n","        x = self._convert_to_tensor_dataset(x).to(self.columns.device)\n","        with torch.no_grad():\n","\n","            outputs = self.columns(x, column_id)\n","\n","            if not self.loss_on_seq:\n","\n","                outputs = get_samples_outputs(outputs)\n","\n","            pred, _ = get_pred_from_outputs(outputs)\n","            pred = pred.detach().cpu().numpy()\n","\n","            if first_train:\n","                return np.concatenate([np.array([None for _ in range(self.seq_len-1)]), pred], axis=0)\n","            return pred\n","\n","    def predict_one (self, x : np.array, column_id: int = None, previous_data_points: np.array = None):\n","        \"\"\"\n","        It performs prediction on a single data point using the last column of cPNN architecture.\n","\n","        Parameters\n","        ----------\n","        x: numpy.array or list\n","            The features values of the single data point.\n","        column_id: int, default: None.\n","            The id of the column to use. If None the last column is used.\n","        previous_data_points: numpy.array, default: None.\n","            The features value of the data points preceding x in the sequence.\n","            If None, it uses the last seq_len-1 points seen during the last calls of the method.\n","            It returns None if the model has not seen yet seq_len-1 data points and previous_data_points is None.\n","        Returns\n","        -------\n","        prediction : int\n","            The predicted int label of x.\n","        \"\"\"\n","        x = np.array(x).reshape(1, -1)\n","        if previous_data_points is not None:\n","            self.previous_data_points_anytime_inference = previous_data_points\n","        if self.previous_data_points_anytime_inference is None:\n","            self.previous_data_points_anytime_inference = x\n","            return None\n","        if len(self.previous_data_points_anytime_inference) != self.seq_len - 1:\n","            self.previous_data_points_anytime_inference = np.concatenate([self.previous_data_points_anytime_inference, x])\n","            return None\n","        self.previous_data_points_anytime_inference = np.concatenate([self.previous_data_points_anytime_inference, x])\n","        x = self._convert_to_tensor_dataset(self.previous_data_points_anytime_inference).to(self.columns.device)\n","        self.previous_data_points_anytime_inference = self.previous_data_points_anytime_inference[1:]\n","        with torch.no_grad():\n","            if not self.loss_on_seq:\n","                pred, _ = get_pred_from_outputs(self.columns(x, column_id)[0])\n","            else:\n","                pred, _ = get_pred_from_outputs(self.columns(x, column_id))\n","            return int(pred[-1].detach().cpu().numpy())\n","\n","    def get_n_columns(self):\n","        return len(self.columns.columns)\n","\n","    def reset_previous_data_points(self):\n","        self.previous_data_points_batch_train = None\n","        self.previous_data_points_anytime_train = None\n","        self.previous_data_points_anytime_inference = None\n","\n","    def test_many_anytime(self, x: np.array, y: np.array, column_id: int = None) -> dict:\n","        \"\"\"\n","        It tests cPNN on a single batch, by computing the metrics after averaging each data point's predictions.\n","        Each prediction is made on the single data point individually.\n","\n","        Parameters\n","        ----------\n","        x: numpy.array\n","            The features values of batch.\n","        y: numpy.array\n","            The real int label of the batch.\n","        column_id: int, default: None.\n","            The id of the column to use. If None the last column is used.\n","\n","        Returns\n","        -------\n","        perf: dict\n","            A dictionary containing the evaluated metrics.\n","        \"\"\"\n","        y_pred = [self.predict_one(x_, column_id=column_id) for x_ in x]\n","        y = np.array([y[i] for i in range(len(y_pred)) if y_pred[i] is not None])\n","        y_pred = np.array([y_ for y_ in y_pred if y_ is not None])\n","        if len(y_pred) == 0:\n","            return {k: None for k in [\"accuracy\", \"kappa\", \"kappa_temporal\"]}\n","        return {\n","            \"accuracy\" : accuracy_score(y, y_pred),\n","            \"kappa\" : cohen_kappa_score(y, y_pred),\n","            \"kappa_temporal\" : kappa_temporal_score(y, y_pred, self.first_label_kappa)\n","        }\n","\n","    def test_many(self, x: np.array, y: np.array, column_id: int = None) -> dict:\n","        \"\"\"\n","        It tests cPNN on a single batch, by computing the metrics after averaging each data point's predictions.\n","        *ONLY FOR BATCH LEARNER*\n","\n","        Parameters\n","        ----------\n","        x: numpy.array\n","            The features values of the batch.\n","        y: numpy.array\n","            The target values of the batch.\n","        column_id: int, default: None.\n","            The id of the column to use. If None the last column is used.\n","\n","        Returns\n","        -------\n","        perf_test: dict\n","            The dictionary representing test's performance.\n","            The following metrics are computed: accuracy, kappa, kappa_temporal.\n","        \"\"\"\n","        if self.anytime_learner:\n","            warnings.warn(\n","                \"The model is an anytime learner, it cannot learn from batch.\\n\" +\n","                \"You cannot call this method\"\n","            )\n","            return {}\n","        if x.shape[0] < self.seq_len:\n","            return {k: None for k in [\"accuracy\", \"kappa\", \"kappa_temporal\"]}\n","\n","        y_pred = self.predict_many(x, column_id)\n","        perf = {\n","            \"accuracy\": accuracy_score(y, y_pred),\n","            \"kappa\": cohen_kappa_score(y, y_pred),\n","            \"kappa_temporal\": kappa_temporal_score(y, y_pred, self.first_label_kappa)\n","        }\n","\n","        if self.train_verbose:\n","            print(f\"Test accuracy: {perf['accuracy']}\")\n","        return perf\n","\n","    def test_then_train(\n","        self,\n","        x: np.array,\n","        y: np.array,\n","        column_id: int = None,\n","    ) -> tuple:\n","        \"\"\"\n","        It tests cPNN on a single batch, and then it performs the training.\n","        It computes the loss after averaging each sample's predictions.\n","        *ONLY FOR BATCH LEARNER*\n","\n","        Parameters\n","        ----------\n","        x: numpy.array\n","            The features values of the batch.\n","        y: numpy.array\n","            The target values of the batch.\n","        column_id: int, default: None.\n","            The id of the column to use for test. If None the last column is used.\n","\n","        Returns\n","        -------\n","        perf_test: dict\n","            The dictionary representing test's performance on the batch.\n","            The following metrics are computed: accuracy, loss, kappa, kappa_temporal.\n","        perf_test_single_pred: dict\n","            The dictionary representing test's performance on the batch by predicting of data point's label individually.\n","            The following metrics are computed: accuracy, kappa, kappa_temporal.\n","        perf_train: dict\n","            The dictionary representing training's performance on the batch.\n","            For each metric the dict contains a list of epochs' values.\n","            The following metrics are computed: accuracy, loss, kappa, kappa_temporal.\n","        \"\"\"\n","        if self.anytime_learner:\n","            warnings.warn(\n","                \"The model is an anytime learner, it cannot learn from batch.\\n\" +\n","                \"You cannot call this method\"\n","            )\n","            return ()\n","\n","        perf_test_single_pred = self.test_many_anytime(x, y)\n","\n","        perf_test = self.test_many(x, y, column_id)\n","\n","        perf_train = self.learn_many(x, y)\n","\n","        self.first_label_kappa = torch.tensor(y[-1]).view(1)\n","\n","        return perf_test, perf_test_single_pred, perf_train\n","\n","    def pretraining(\n","        self, x: np.array, y: list, epochs: int = 100, batch_size: int = 128\n","    ) -> dict:\n","        \"\"\"\n","        It performs the pretraining on a pretraining set.\n","        *ONLY FOR BATCH LEARNER*\n","\n","        Parameters\n","        ----------\n","        x: numpy.array\n","            The features values of the set.\n","        y: list\n","            The target values of the set.\n","        epochs: int, default: 100.\n","            The number of training epochs to perform on the set.\n","        batch_size: int, default: 128.\n","            The training batch size.\n","\n","        Returns\n","        -------\n","        perf_train: dict\n","            The dictionary representing training's performance.\n","            The following metrics are computed: accuracy, loss, kappa, kappa_temporal.\n","            For each metric the dict contains a list of shape (epochs, n_batches) where n_batches is the training\n","            batches number.\n","        \"\"\"\n","        if self.anytime_learner:\n","            warnings.warn(\n","                \"The model is an anytime learner, it cannot learn from batch.\\n\" +\n","                \"You cannot call this method\"\n","            )\n","            return {}\n","\n","        perf_train = {\n","            \"accuracy\": [],\n","            \"loss\": [],\n","            \"kappa\": [],\n","            \"kappa_temporal\": [],\n","        }\n","\n","        x = torch.tensor(x)\n","        y = torch.tensor(y).type(torch.LongTensor)\n","        data = data_utils.TensorDataset(x, y)\n","        loader = DataLoader(data, batch_size=batch_size, drop_last=False)\n","        print(\"Pretraining\")\n","        for e in range(1, epochs + 1):\n","            for k in perf_train:\n","                perf_train[k].append([])\n","            for id_batch, (x, y) in enumerate(loader):\n","                print(\n","                    f\"{id_batch+1}/{len(loader)} batch of {e}/{epochs} epoch\", end=\"\\r\"\n","                )\n","                x, y_seq = self._cut_in_sequences_tensors(x, y)\n","                perf_batch = self._fit(x, y)\n","                for k in perf_batch:\n","                    perf_train[k][-1].append(perf_batch[k])\n","        print()\n","        print()\n","        return perf_train\n","\n","    def _fit(self, x, y):\n","        x, y = x.to(self.columns.device), y.to(self.columns.device)\n","        print(x.shape)\n","        print('inside the fit model')\n","        outputs = self.columns(x, train=True)\n","        if not self.loss_on_seq:\n","            outputs = get_samples_outputs(outputs)\n","        loss = customized_loss(outputs, y, self.columns.criterion)\n","        self.columns.optimizers[-1].zero_grad()\n","        loss.backward()\n","        self.columns.optimizers[-1].step()\n","        outputs = self.columns(x)\n","        if not self.loss_on_seq:\n","            outputs = get_samples_outputs(outputs)\n","        perf_train = {\n","            \"loss\": loss.item(),\n","            \"accuracy\": accuracy(outputs, y).item(),\n","            \"kappa\": cohen_kappa(outputs, y, device=self.columns.device).item(),\n","            \"kappa_temporal\": kappa_temporal(outputs, y, self.first_label_kappa).item(),\n","        }\n","        return perf_train\n","\n","    def get_hidden(self, x, column_id=None):\n","        return self.columns.get_hidden(x, column_id)\n"],"metadata":{"id":"KO8vWViG_5yW","executionInfo":{"status":"ok","timestamp":1698223375028,"user_tz":-120,"elapsed":596,"user":{"displayName":"reza paki","userId":"01979157922216683878"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(acc).to_csv('results/acc-cpnn-16_16-2341.csv', index=False)\n","pd.DataFrame(kappa).to_csv('results/kappa-cpnn-16_16-2341.csv', index=False)"],"metadata":{"id":"OGXbc-uC6Cq7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##This is for running CPNN with gru and pretrain model based on train data"],"metadata":{"id":"SPeE-nJDQPvt"}},{"cell_type":"code","source":["\n","# EDITABLE PARAMETERS\n","dataset1 = \"sine_rw10_mode5_extended_6-6_1234\"\n","pretrain_data = pd.read_csv(os.path.join(\"datasets/datasets\", f\"{dataset1}.csv\"))\n","pretrain_data = pretrain_data[pretrain_data[\"task\"] == 4]\n","pretrain_data.loc[:, 'task'] = 0\n","dataset2 = \"sine_rw10_mode5_extended_16-16_2341\"\n","\n","# OTHER PARAMETERS\n","batch_size = 128\n","hidden_size = 50\n","seq_len = 10\n","# TODO\n","iterations = 1\n","loss_on_seq = False\n","freeze_inputs_weights = False\n","pretraining_samples = 0\n","pretraining_epochs = 0\n","write_weights = False\n","combination = False\n","rembember_initial_states = False\n","suffix = \"\"\n","\n","if freeze_inputs_weights:\n","    suffix += \"_exp\"\n","if combination:\n","    suffix = \"_combination\" + suffix\n","if suffix != \"\" and suffix[0:1] != \"_\":\n","    suffix = \"_\" + suffix\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\n","    \"--model\",\n","    type=str,\n","    default=\"cpnn\",\n","    help=\"Model to use: {'cpnn', 'single': cLSTM, 'multiple': mcLSTM}\",\n",")\n","parser.add_argument(\n","    \"--model_class\",\n","    type=str,\n","    default=\"cgru\",\n","    help=\"Base learner to use: {'clstm', 'cgru'}\",\n",")\n","args,_ = parser.parse_known_args()\n","\n","if args.model_class == \"clstm\":\n","    model_class = cLSTMLinear\n","else:\n","    model_class = cGRULinear\n","if hidden_size is None:\n","    if args.model_class == \"clstm\":\n","        hidden_size = 50\n","    else:\n","        hidden_size = 128\n","device = torch.device(\"cpu\")\n","df = pd.read_csv(os.path.join(\"datasets/datasets\", f\"{dataset2}.csv\"))\n","\n","df = pretrain_data.append(df, ignore_index=True)\n","perf_test = {\"accuracy\": [], \"kappa\": [], \"kappa_temporal\": [], \"loss\": []}\n","perf_train = {\"accuracy\": [], \"kappa\": [], \"kappa_temporal\": [], \"loss\": []}\n","perf_anytime = {\"accuracy\": [], \"kappa\": [], \"kappa_temporal\": []}\n","seq_str = \"_seq\" if loss_on_seq else \"\"\n","\n","path = os.path.join(\n","    \"performance\",\n","    f\"{dataset2}/{args.model}_{args.model_class}{seq_str}{suffix}_{hidden_size}hs\",\n",")\n","if not os.path.isdir(path):\n","    os.makedirs(path)\n","\n","path_anytime = path + \"_anytime\"\n","if not os.path.isdir(path_anytime):\n","    os.makedirs(path_anytime)\n","\n","# UTILS\n","print(model_class)\n","def create_cpnn():\n","    if not loss_on_seq:\n","        if not freeze_inputs_weights:\n","            return cPNN(column_class=model_class, device=device, seq_len=seq_len, train_verbose=False,\n","                        combination=combination, input_size=len(df.columns) - 2, hidden_size=hidden_size, output_size=2,\n","                        batch_size=batch_size)\n","        else:\n","            return cPNNExp(\n","                column_class=model_class,\n","                input_size=len(df.columns) - 2,\n","                hidden_size=hidden_size,\n","                output_size=2,\n","                batch_size=batch_size,\n","                device=device,\n","                seq_len=seq_len,\n","                train_verbose=False,\n","                combination=combination,\n","                remember_initial_states=rembember_initial_states,\n","            )\n","    return cPNNSeq(\n","        column_class=model_class,\n","        input_size=len(df.columns) - 2,\n","        hidden_size=hidden_size,\n","        output_size=2,\n","        batch_size=batch_size,\n","        device=device,\n","        seq_len=seq_len,\n","    )\n","\n","\n","# MAIN\n","if __name__ == \"__main__\":\n","    if args.model == \"cpnn\" and write_weights:\n","        try:\n","            df_test = pd.read_csv(os.path.join(\"datasets/datasets\", f\"{dataset}_test.csv\"))\n","        except:\n","            pass\n","    models = []\n","    params = []\n","    inputs = []\n","    hiddens = []\n","    acc=[]\n","    kappa=[]\n","\n","    print(dataset2)\n","    for i in range(1, iterations + 1):\n","        models.append([])\n","        params.append([])\n","        inputs.append([])\n","        hiddens.append([])\n","\n","        for k in perf_test:\n","            perf_test[k].append([])\n","        for k in perf_train:\n","            perf_train[k].append([])\n","        for k in perf_anytime:\n","            perf_anytime[k].append([])\n","        models[-1].append(create_cpnn())\n","        print(type(models[-1][-1].columns.columns[0]).__name__)\n","        print(f\"{i}/{iterations} iteration of {args.model}\")\n","        for task in range(0, df[\"task\"].max() + 1):\n","            acc.append([])\n","            kappa.append([])\n","            params[-1].append([])\n","            inputs[-1].append([])\n","            hiddens[-1].append([])\n","            print(\"TASK:\", task)\n","            if task > 0:\n","                if args.model == \"cpnn\":\n","                    models[-1][-1].add_new_column()\n","                elif args.model == \"multiple\":\n","                    models[-1].append(create_cpnn())\n","                elif args.model == \"single\":\n","                    models[-1].append(pickle.loads(pickle.dumps(models[-1][-1])))\n","            df_task = df[df[\"task\"] == task]\n","            #df_task.loc[:, 'task'] = 1\n","            df_task = df_task.drop(columns=\"task\")\n","            if pretraining_samples > 0:\n","                df_pre = df_task.iloc[:pretraining_samples, 0:]\n","                df_task = df_task.iloc[pretraining_samples:, 0:]\n","                perf_pretraining = models[-1][-1].pretraining(\n","                    df_pre.iloc[0:, :-1].values.astype(np.float32),\n","                    list(df_pre.iloc[0:, -1]),\n","                    pretraining_epochs,\n","                )\n","                with open(\n","                    os.path.join(path, \"pretraining.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(perf_pretraining, f)\n","\n","            for k in perf_test:\n","                perf_test[k][-1].append([])\n","            for k in perf_train:\n","                perf_train[k][-1].append([])\n","            for k in perf_anytime:\n","                perf_anytime[k][-1].append([])\n","            if len(df_task) % batch_size == 0:\n","                n_batches = int(len(df_task) / batch_size)\n","            else:\n","                n_batches = int(len(df_task) / batch_size) + 1\n","            for i in range(0, len(df_task), batch_size):\n","                x = df_task.iloc[i : i + batch_size, 0:-1].values.astype(np.float32)\n","                y = list(df_task.iloc[i : i + batch_size, -1])\n","                print(int(i / batch_size) + 1, \"/\", n_batches, \" batch\", end=\"\\r\")\n","                if len(y) >= seq_len:\n","                    batch_perf_test, batch_perf_anytime, batch_perf_train = models[-1][-1].test_then_train(\n","                        x, y\n","                    )\n","                    acc[-1].append([batch_perf_test['accuracy']])\n","                    kappa[-1].append([batch_perf_test['kappa']])\n","                    for k in batch_perf_test:\n","                        perf_test[k][-1][-1].append(batch_perf_test[k])\n","                    for k in batch_perf_anytime:\n","                        perf_anytime[k][-1][-1].append(batch_perf_anytime[k])\n","                    for k in batch_perf_train:\n","                        perf_train[k][-1][-1].append(batch_perf_train[k])\n","                    if args.model == \"cpnn\" and write_weights:\n","                        try:\n","                            df_test_task = df_test[df_test[\"task\"] == task].drop(\n","                                columns=\"task\"\n","                            )\n","                            x_test = df_test_task.iloc[0:500, 0:-1].values.astype(\n","                                np.float32\n","                            )\n","                            inputs[-1][-1].append(\n","                                models[-1][-1]\n","                                .columns._convert_to_tensor_dataset(x_test)\n","                                .detach()\n","                                .numpy()\n","                            )\n","                            hiddens[-1][-1].append(models[-1][-1].get_hidden(x_test))\n","                        except:\n","                            pass\n","                        params[-1][-1].append(\n","                            pickle.loads(\n","                                pickle.dumps(\n","                                    models[-1][-1]\n","                                    .columns.columns[-1]\n","                                    .lstm.weight_ih_l0.data.detach()\n","                                    .numpy()\n","                                )\n","                            )\n","                        )\n","            print()\n","            print(\n","                f\"Accuracy media sul task {task}: {np.mean(perf_test['accuracy'][-1][-1])}\"\n","            )\n","            print()\n","\n","            with open(\n","                os.path.join(path, \"test_then_train.pkl\"),\n","                \"wb\",\n","            ) as f:\n","                pickle.dump(perf_test, f)\n","\n","            with open(\n","                os.path.join(path_anytime, \"test_then_train.pkl\"),\n","                \"wb\",\n","            ) as f:\n","                pickle.dump(perf_anytime, f)\n","\n","            with open(\n","                os.path.join(path, \"train.pkl\"),\n","                \"wb\",\n","            ) as f:\n","                pickle.dump(perf_train, f)\n","            with open(os.path.join(path, \"models.pkl\"), \"wb\") as f:\n","                pickle.dump(models, f)\n","\n","            if args.model == \"cpnn\" and write_weights:\n","                with open(\n","                    os.path.join(path, \"cpnn_params.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(params, f)\n","\n","                with open(\n","                    os.path.join(path, \"inputs.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(inputs, f)\n","\n","                with open(\n","                    os.path.join(path, \"hiddens.pkl\"),\n","                    \"wb\",\n","                ) as f:\n","                    pickle.dump(hiddens, f)\n","        print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRP85JMtI3fq","executionInfo":{"status":"ok","timestamp":1697893525684,"user_tz":-120,"elapsed":912509,"user":{"displayName":"reza paki","userId":"01979157922216683878"}},"outputId":"42376ea4-8341-4014-c4bd-8cc1ca618b60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-ea416d7c5c80>:56: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df = pretrain_data.append(df, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<class 'models.cgru.cGRULinear'>\n","sine_rw10_mode5_extended_16-16_2341\n","cGRULinear\n","1/1 iteration of cpnn\n","TASK: 0\n","391 / 391  batch\n","Accuracy media sul task 0: 0.8835078324808184\n","\n","TASK: 1\n","\n","Accuracy media sul task 1: 0.8623801150895141\n","\n","TASK: 2\n","\n","Accuracy media sul task 2: 0.8452765345268543\n","\n","TASK: 3\n","\n","Accuracy media sul task 3: 0.9094309462915601\n","\n","TASK: 4\n","\n","Accuracy media sul task 4: 0.9209119245524298\n","\n","\n"]}]},{"cell_type":"code","source":["pd.DataFrame(acc).to_csv('results/acc-cpnn-16_16-2341-pretrain-t4.csv', index=False)\n","pd.DataFrame(kappa).to_csv('results/kappa-cpnn-16_16-2341-pretrain-t4.csv', index=False)"],"metadata":{"id":"TetdePx3YRce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"H91lnJTCwVas"}},{"cell_type":"code","source":[],"metadata":{"id":"sdYnMno_gRo6"},"execution_count":null,"outputs":[]}]}