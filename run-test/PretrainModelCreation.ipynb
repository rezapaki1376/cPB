{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","os.chdir('..')\n","sys.path.append(os.getcwd())"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gpYHkmOlsv9m"},"outputs":[],"source":["import torch\n","from utils.preprocess import *\n","\n","from Models.pretrain import *\n","from utils.utils import (\n","    cohen_kappa,\n","    get_samples_outputs,\n","    get_pred_from_outputs,\n",")\n","\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import argparse\n","from torch.autograd import Variable\n","import copy\n"]},{"cell_type":"markdown","metadata":{"id":"Q3SOVmoXDVKd"},"source":["# Weather datasets\n"]},{"cell_type":"markdown","metadata":{},"source":["## CONFIGURATION:\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# TODO\n","dataset='Weather'\n","dataset_name = \"weather_pretraining\"\n","batch_size = 128\n","hidden_size = 250\n","seq_len = 11\n","iterations = 10\n","output_size = 2\n","num_layers = 1\n","loss_on_seq = False\n","freeze_inputs_weights = False\n","pretraining_samples = 0\n","pretraining_epochs = 0\n","write_weights = False\n","combination = False\n","model_type = 'GRU' # or LSTM\n","if model_type == \"GRU\":\n","    model_class = GRU_Model\n","elif model_type == \"LSTM\":\n","    model_class = LSTM_Model\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Executable code:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(f\"datasets/{dataset}/{dataset_name}.csv\")\n","df_task = df.drop('task', axis=1)\n","df_task"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":493226,"status":"ok","timestamp":1710593991089,"user":{"displayName":"reza paki","userId":"01979157922216683878"},"user_tz":-60},"id":"9WzgTQq9vnif","outputId":"fcc7805b-6aa6-40c2-9ba7-65f6a8c0bdbc"},"outputs":[],"source":["for itter in range(1,iterations+1):\n","  Model = model_class(input_size=4,\n","        device=torch.device(\"cpu\"),\n","      \tnum_layers=num_layers,\n","        hidden_size=hidden_size,\n","        output_size=output_size,\n","        batch_size=batch_size,\n","          )\n","  Preprocess_object = Preprocess(seq_len=seq_len)\n","  with open(f\"Performance/Pretrain/{model_type}/{dataset}/Before/{dataset}-{model_type}-pretrain-hidden{hidden_size}-epoch10_itter{itter}.pickle\", \"wb\") as fp:\n","    pickle.dump(Model.state_dict(), fp)\n","\n","  if len(df_task) % batch_size == 0:\n","    n_batches = int(len(df_task) / batch_size)\n","  else:\n","    n_batches = int(len(df_task) / batch_size) + 1\n","  optimizer = torch.optim.Adam(Model.parameters(), lr=0.01)\n","  loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n","  out_h = None\n","  for j in range(0,10):\n","    for i in range(0, len(df_task), batch_size):\n","      x = df_task.iloc[i : i + batch_size, 0:-1].values.astype(np.float32)\n","      y = list(df_task.iloc[i : i + batch_size, -1])\n","      if len(y) >= seq_len:\n","        x = np.array(x)\n","        y = list(y)\n","        x, y, _ = Preprocess_object._load_batch(x, y)\n","        y_pred = Model(x)\n","        y_pred = get_samples_outputs(y_pred)\n","        pred, _ = get_pred_from_outputs(y_pred)\n","        loss = loss_fn(y_pred, y)\n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        # update weights\n","        optimizer.step()\n","  with open(f\"Performance/Pretrain/{model_type}/{dataset}/After/{dataset}-{model_type}-pretrain-hidden{hidden_size}-epoch10_itter{itter}.pickle\", \"wb\") as fp:\n","    pickle.dump(Model.state_dict(), fp)\n","  print(f'itter {itter} finished')"]},{"cell_type":"markdown","metadata":{},"source":["# SINE datasets\n"]},{"cell_type":"markdown","metadata":{},"source":["## CONFIGURATION:\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"j5MRnZ4-lZ-8"},"outputs":[],"source":["# TODO\n","dataset='SINE'\n","dataset_name = \"sine_rw10_mode5_extended_6-6_1234\"\n","Task_Number = 1\n","batch_size = 128\n","hidden_size = 250\n","seq_len = 10\n","iterations = 10\n","output_size = 2\n","num_layers = 1\n","loss_on_seq = False\n","freeze_inputs_weights = False\n","pretraining_samples = 0\n","pretraining_epochs = 0\n","write_weights = False\n","combination = False\n","model_type = 'GRU' # or LSTM\n","if model_type == \"GRU\":\n","    model_class = GRU_Model\n","elif model_type == \"LSTM\":\n","    model_class = LSTM_Model\n"]},{"cell_type":"markdown","metadata":{},"source":["## Executable code:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AqcFv6Y8IzMk"},"outputs":[],"source":["df = pd.read_csv(f\"datasets/{dataset}/{dataset_name}.csv\")\n","df_task = df[df[\"task\"] == Task_Number]\n","df_task = df_task.drop('task', axis=1)\n","df_task"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for itter in range(1,iterations+1):\n","  Model = model_class(input_size=4,\n","        device=torch.device(\"cpu\"),\n","      \tnum_layers=num_layers,\n","        hidden_size=hidden_size,\n","        output_size=output_size,\n","        batch_size=batch_size,\n","          )\n","  Preprocess_object = Preprocess(seq_len=seq_len)\n","  with open(f\"Performance/Pretrain/{model_type}/{dataset}/Before/{dataset}-{model_type}-pretrain-hidden{hidden_size}-epoch10_iter{itter}.pickle\", \"wb\") as fp:\n","    pickle.dump(Model.state_dict(), fp)\n","\n","  if len(df_task) % batch_size == 0:\n","    n_batches = int(len(df_task) / batch_size)\n","  else:\n","    n_batches = int(len(df_task) / batch_size) + 1\n","  optimizer = torch.optim.Adam(Model.parameters(), lr=0.01)\n","  loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n","  out_h = None\n","  for j in range(0,10):\n","    for i in range(0, len(df_task), batch_size):\n","      x = df_task.iloc[i : i + batch_size, 0:-1].values.astype(np.float32)\n","      y = list(df_task.iloc[i : i + batch_size, -1])\n","      if len(y) >= seq_len:\n","        x = np.array(x)\n","        y = list(y)\n","        x, y, _ = Preprocess_object._load_batch(x, y)\n","        y_pred = Model(x)\n","        y_pred = get_samples_outputs(y_pred)\n","        pred, _ = get_pred_from_outputs(y_pred)\n","        loss = loss_fn(y_pred, y)\n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        # update weights\n","        optimizer.step()\n","  with open(f\"Performance/Pretrain/{model_type}/{dataset}/After/{dataset_name}-{model_type}-pretrain-hidden{hidden_size}-epoch10_iter{itter}.pickle\", \"wb\") as fp:\n","    pickle.dump(Model.state_dict(), fp)\n","  print(f'itter {itter} finished')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM82h1t6isZ5UsHdGaHl7jp","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"}},"nbformat":4,"nbformat_minor":0}
