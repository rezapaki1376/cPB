{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","os.chdir('..')\n","sys.path.append(os.getcwd())"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import torch\n","from Models.pretrain import *\n","from Models.cPB import cPB\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import argparse\n","from torch.autograd import Variable\n","import pprint\n","import copy\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning, module='torch.storage')\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch.storage')\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module='torch.storage')"]},{"cell_type":"markdown","metadata":{},"source":["# SINE datasets\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["base_model = 'GRU'\n","batch_size = 128\n","hidden_size = 250\n","seq_len = 10\n","epoch_size=10\n","lr = 0.01\n","number_of_tasks=4\n","mask_selection_NofBatch = 50\n","input_size=2\n","iteration=1\n","mask_init='uniform'\n","dataset='SINE'\n","dataset_name = \"sine_rw10_mode5_extended_16-16_1234\"\n","Pretrain_task = '1'\n","\n","df = pd.read_csv(os.path.join(f\"datasets/{dataset}/\", f\"{dataset_name}.csv\"))\n","pretrain_model_addr=f'Performance/Pretrain/GRU/{dataset}/After/sine-6_6-1234-t{Pretrain_task}-{base_model}-pretrain-hidden{hidden_size}-epoch10_itter{iteration}.pickle'\n","mask_weights=[] #if we have initial masks then reload it here\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model=cPB(lr = lr ,hidden_size=hidden_size, seq_len=seq_len,base_model=base_model, pretrain_model_addr=pretrain_model_addr,\n","           mask_weights=mask_weights, mask_init=mask_init, number_of_tasks=number_of_tasks,epoch_size=epoch_size,input_size=input_size)\n","\n","\n","for iter in range(0,iteration):\n","  # loop for each task\n","  for task in range(1, df[\"task\"].max() + 1):\n","    df_task = df[df[\"task\"] == task]\n","    df_task = df_task.drop(columns=\"task\")\n","    # loop based on each batch of data\n","    batch_cont=0\n","    model.weights_copy(task)\n","    for i in range(0, len(df_task), batch_size):\n","      x = df_task.iloc[i : i + batch_size, 0:-1].values.astype(np.float32)\n","      y = list(df_task.iloc[i : i + batch_size, -1])\n","      if batch_cont<mask_selection_NofBatch:\n","        for mask_index in range (0, task):\n","          model.predict_many(x,y,mask_index,task,mask_selection=True)\n","          model.learn_many(x,y,mask_index)\n","        batch_cont+=1\n","      elif batch_cont==mask_selection_NofBatch:\n","        best_mask_index=model.add_new_column(task)\n","        model.predict_many(x,y,best_mask_index,task,mask_selection=False)\n","        model.learn_many(x,y,best_mask_index)\n","        batch_cont+=1\n","      elif batch_cont>mask_selection_NofBatch:\n","        batch_cont+=1\n","        model.predict_many(x,y,best_mask_index,task,mask_selection=False)\n","        model.learn_many(x,y,best_mask_index)\n","    model.save_final_metrics(task,best_mask_index)\n","    model.final_weights_saving()\n","  model.plotting()\n","\n","\n","file_path = f'Performance/Results/cPB/{dataset}/pretrain-T{Pretrain_task}_{base_model}-{dataset_name}-hidden{hidden_size}-epoch10_iter{iteration}.pkl'\n","with open(file_path, 'wb') as file:\n","    pickle.dump(model.performance, file)\n","\n","file_path = f'Performance/Results/cPB/{dataset}/pretrain-T{Pretrain_task}_{base_model}-{dataset_name}-hidden{hidden_size}-epoch10_iter{iteration}_selectedMask.pkl'\n","with open(file_path, 'wb') as file:\n","    pickle.dump(model.selected_mask_index, file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["base_model = 'GRU'\n","batch_size = 128\n","hidden_size = 250\n","seq_len = 11\n","epoch_size=10\n","lr = 0.01\n","number_of_tasks=4\n","mask_selection_NofBatch = 50\n","input_size=2\n","iteration=1\n","mask_init='uniform'\n","dataset='Weather'\n","dataset_name = \"weather_st124_4conf\"\n","Pretrain_task = '1'\n","\n","df = pd.read_csv(os.path.join(f\"datasets/{dataset}/\", f\"{dataset_name}.csv\"))\n","pretrain_model_addr=f'Performance/Pretrain/GRU/{dataset}/After/weather-{base_model}-pretrain-hidden{hidden_size}-epoch10_itter{iteration}.pickle'\n","mask_weights=[] #if we have initial masks then reload it here\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for GRU_Model:\n\tsize mismatch for gru.weight_ih_l0: copying a param with shape torch.Size([750, 4]) from checkpoint, the shape in current model is torch.Size([750, 2]).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39mcPB(lr \u001b[38;5;241m=\u001b[39m lr ,hidden_size\u001b[38;5;241m=\u001b[39mhidden_size, seq_len\u001b[38;5;241m=\u001b[39mseq_len,base_model\u001b[38;5;241m=\u001b[39mbase_model, pretrain_model_addr\u001b[38;5;241m=\u001b[39mpretrain_model_addr,\n\u001b[1;32m      2\u001b[0m            mask_weights\u001b[38;5;241m=\u001b[39mmask_weights, mask_init\u001b[38;5;241m=\u001b[39mmask_init, number_of_tasks\u001b[38;5;241m=\u001b[39mnumber_of_tasks,epoch_size\u001b[38;5;241m=\u001b[39mepoch_size,input_size\u001b[38;5;241m=\u001b[39minput_size)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,iteration):\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;66;03m# loop for each task\u001b[39;00m\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n","File \u001b[0;32m~/Documents/GitHub/cPB/Models/cPB.py:69\u001b[0m, in \u001b[0;36mcPB.__init__\u001b[0;34m(self, model_class, hidden_size, device, stride, lr, seq_len, base_model, pretrain_model_addr, mask_weights, mask_init, number_of_tasks, epoch_size, input_size, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperformance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrain_model_addr\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 69\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m ModifiedRNN(pretrain_model_addr\u001b[38;5;241m=\u001b[39mpretrain_model_addr,hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,base_model\u001b[38;5;241m=\u001b[39mbase_model,seq_len\u001b[38;5;241m=\u001b[39mseq_len,mask_weights\u001b[38;5;241m=\u001b[39mmask_weights,mask_init\u001b[38;5;241m=\u001b[39mmask_init,input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size)\n\u001b[1;32m     70\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_models_weight\u001b[38;5;241m.\u001b[39mappend([])\n","File \u001b[0;32m~/Documents/GitHub/cPB/Models/network.py:146\u001b[0m, in \u001b[0;36mModifiedRNN.__init__\u001b[0;34m(self, input_size, device, num_layers, hidden_size, output_size, batch_size, base_model, many_to_one, remember_states, bias, dropout, training, bidirectional, batch_first, mask_init, mask_scale, threshold_fn, threshold, pretrain_model_addr, seq_len, mask_weights)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Load pretrained weights\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrain_model_addr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrain_model\u001b[38;5;241m.\u001b[39mload_state_dict(pickle\u001b[38;5;241m.\u001b[39mload(fp), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrain_model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Initialize the piggyback model\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/envs/CPB/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GRU_Model:\n\tsize mismatch for gru.weight_ih_l0: copying a param with shape torch.Size([750, 4]) from checkpoint, the shape in current model is torch.Size([750, 2])."]}],"source":["model=cPB(lr = lr ,hidden_size=hidden_size, seq_len=seq_len,base_model=base_model, pretrain_model_addr=pretrain_model_addr,\n","           mask_weights=mask_weights, mask_init=mask_init, number_of_tasks=number_of_tasks,epoch_size=epoch_size,input_size=input_size)\n","\n","\n","for iter in range(0,iteration):\n","  # loop for each task\n","  for task in range(1, df[\"task\"].max() + 1):\n","    df_task = df[df[\"task\"] == task]\n","    df_task = df_task.drop(columns=\"task\")\n","    # loop based on each batch of data\n","    batch_cont=0\n","    model.weights_copy(task)\n","    for i in range(0, len(df_task), batch_size):\n","      x = df_task.iloc[i : i + batch_size, 0:-1].values.astype(np.float32)\n","      y = list(df_task.iloc[i : i + batch_size, -1])\n","      if batch_cont<mask_selection_NofBatch:\n","        for mask_index in range (0, task):\n","          model.predict_many(x,y,mask_index,task,mask_selection=True)\n","          model.learn_many(x,y,mask_index)\n","        batch_cont+=1\n","      elif batch_cont==mask_selection_NofBatch:\n","        best_mask_index=model.add_new_column(task)\n","        model.predict_many(x,y,best_mask_index,task,mask_selection=False)\n","        model.learn_many(x,y,best_mask_index)\n","        batch_cont+=1\n","      elif batch_cont>mask_selection_NofBatch:\n","        batch_cont+=1\n","        model.predict_many(x,y,best_mask_index,task,mask_selection=False)\n","        model.learn_many(x,y,best_mask_index)\n","    model.save_final_metrics(task,best_mask_index)\n","    model.final_weights_saving()\n","  model.plotting()\n","\n","\n","file_path = f'Performance/Results/cPB/{dataset}/pretrain-T{Pretrain_task}_{base_model}-{dataset_name}-hidden{hidden_size}-epoch10_iter{iteration}.pkl'\n","with open(file_path, 'wb') as file:\n","    pickle.dump(model.performance, file)\n","\n","file_path = f'Performance/Results/cPB/{dataset}/pretrain-T{Pretrain_task}_{base_model}-{dataset_name}-hidden{hidden_size}-epoch10_iter{iteration}_selectedMask.pkl'\n","with open(file_path, 'wb') as file:\n","    pickle.dump(model.selected_mask_index, file)\n"]},{"cell_type":"markdown","metadata":{"id":"Q3SOVmoXDVKd"},"source":["# Weather datasets\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# TODO\n","dataset='Weather'\n","dataset_name = \"weather_pretraining\"\n","batch_size = 128\n","hidden_size = 250\n","seq_len = 11\n","iterations = 10\n","output_size = 2\n","num_layers = 1\n","loss_on_seq = False\n","freeze_inputs_weights = False\n","pretraining_samples = 0\n","pretraining_epochs = 0\n","write_weights = False\n","combination = False\n","model_type = 'GRU' # or LSTM\n","if model_type == \"GRU\":\n","    model_class = GRU_Model\n","elif model_type == \"LSTM\":\n","    model_class = LSTM_Model\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>RH</th>\n","      <th>T_d</th>\n","      <th>w_s</th>\n","      <th>w_d</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.878049</td>\n","      <td>0.180556</td>\n","      <td>0.142857</td>\n","      <td>-1.333083</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.878049</td>\n","      <td>0.138889</td>\n","      <td>0.571429</td>\n","      <td>0.652632</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.926829</td>\n","      <td>0.111111</td>\n","      <td>0.357143</td>\n","      <td>-1.298496</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.975610</td>\n","      <td>0.125000</td>\n","      <td>0.428571</td>\n","      <td>-0.875188</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.926829</td>\n","      <td>0.138889</td>\n","      <td>0.142857</td>\n","      <td>-1.347368</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24100</th>\n","      <td>-0.707317</td>\n","      <td>1.277778</td>\n","      <td>0.428571</td>\n","      <td>-1.346617</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24101</th>\n","      <td>0.560976</td>\n","      <td>2.138889</td>\n","      <td>0.714286</td>\n","      <td>-0.121053</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24102</th>\n","      <td>-0.073171</td>\n","      <td>2.041667</td>\n","      <td>-0.142857</td>\n","      <td>-0.309023</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24103</th>\n","      <td>-0.390244</td>\n","      <td>1.722222</td>\n","      <td>0.214286</td>\n","      <td>-0.128571</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24104</th>\n","      <td>-0.170732</td>\n","      <td>1.916667</td>\n","      <td>0.142857</td>\n","      <td>0.450376</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>24105 rows × 5 columns</p>\n","</div>"],"text/plain":["             RH       T_d       w_s       w_d  target\n","0     -0.878049  0.180556  0.142857 -1.333083       1\n","1     -0.878049  0.138889  0.571429  0.652632       1\n","2     -0.926829  0.111111  0.357143 -1.298496       1\n","3     -0.975610  0.125000  0.428571 -0.875188       1\n","4     -0.926829  0.138889  0.142857 -1.347368       1\n","...         ...       ...       ...       ...     ...\n","24100 -0.707317  1.277778  0.428571 -1.346617       1\n","24101  0.560976  2.138889  0.714286 -0.121053       1\n","24102 -0.073171  2.041667 -0.142857 -0.309023       1\n","24103 -0.390244  1.722222  0.214286 -0.128571       1\n","24104 -0.170732  1.916667  0.142857  0.450376       1\n","\n","[24105 rows x 5 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(f\"datasets/{dataset}/{dataset_name}.csv\")\n","df_task = df.drop('task', axis=1)\n","df_task"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":493226,"status":"ok","timestamp":1710593991089,"user":{"displayName":"reza paki","userId":"01979157922216683878"},"user_tz":-60},"id":"9WzgTQq9vnif","outputId":"fcc7805b-6aa6-40c2-9ba7-65f6a8c0bdbc"},"outputs":[],"source":["for itter in range(1,iterations+1):\n","  Model = model_class(input_size=4,\n","        device=torch.device(\"cpu\"),\n","      \tnum_layers=num_layers,\n","        hidden_size=hidden_size,\n","        output_size=output_size,\n","        batch_size=batch_size,\n","          )\n","  Preprocess_object = Preprocess(seq_len=seq_len)\n","  with open(f\"Performance/Pretrain/{model_type}/{dataset}/Before/{dataset}-{model_type}-pretrain-hidden{hidden_size}-epoch10_itter{itter}.pickle\", \"wb\") as fp:\n","    pickle.dump(Model.state_dict(), fp)\n","\n","  if len(df_task) % batch_size == 0:\n","    n_batches = int(len(df_task) / batch_size)\n","  else:\n","    n_batches = int(len(df_task) / batch_size) + 1\n","  optimizer = torch.optim.Adam(Model.parameters(), lr=0.01)\n","  loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n","  out_h = None\n","  for j in range(0,10):\n","    for i in range(0, len(df_task), batch_size):\n","      x = df_task.iloc[i : i + batch_size, 0:-1].values.astype(np.float32)\n","      y = list(df_task.iloc[i : i + batch_size, -1])\n","      if len(y) >= seq_len:\n","        x = np.array(x)\n","        y = list(y)\n","        x, y, _ = Preprocess_object._load_batch(x, y)\n","        y_pred = Model(x)\n","        y_pred = get_samples_outputs(y_pred)\n","        pred, _ = get_pred_from_outputs(y_pred)\n","        loss = loss_fn(y_pred, y)\n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        # update weights\n","        optimizer.step()\n","  with open(f\"Performance/Pretrain/{model_type}/{dataset}/After/{dataset}-{model_type}-pretrain-hidden{hidden_size}-epoch10_itter{itter}.pickle\", \"wb\") as fp:\n","    pickle.dump(Model.state_dict(), fp)\n","  print(f'itter {itter} finished')"]},{"cell_type":"markdown","metadata":{},"source":["# SINE datasets\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"j5MRnZ4-lZ-8"},"outputs":[],"source":["# TODO\n","dataset='SINE'\n","dataset_name = \"sine_rw10_mode5_extended_6-6_1234\"\n","Task_Number = 1\n","batch_size = 128\n","hidden_size = 250\n","seq_len = 10\n","iterations = 10\n","output_size = 2\n","num_layers = 1\n","loss_on_seq = False\n","freeze_inputs_weights = False\n","pretraining_samples = 0\n","pretraining_epochs = 0\n","write_weights = False\n","combination = False\n","model_type = 'GRU' # or LSTM\n","if model_type == \"GRU\":\n","    model_class = GRU_Model\n","elif model_type == \"LSTM\":\n","    model_class = LSTM_Model\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"AqcFv6Y8IzMk"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x1</th>\n","      <th>x2</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.718354</td>\n","      <td>0.957244</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.725173</td>\n","      <td>0.952800</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.772454</td>\n","      <td>0.922077</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.803595</td>\n","      <td>0.872254</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.148729</td>\n","      <td>0.872254</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>0.431281</td>\n","      <td>0.891558</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>0.391986</td>\n","      <td>0.877517</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>0.417829</td>\n","      <td>0.916618</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>0.115487</td>\n","      <td>0.916618</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>0.163136</td>\n","      <td>0.931397</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows × 3 columns</p>\n","</div>"],"text/plain":["             x1        x2  target\n","0      0.718354  0.957244       0\n","1      0.725173  0.952800       0\n","2      0.772454  0.922077       0\n","3      0.803595  0.872254       0\n","4      0.148729  0.872254       0\n","...         ...       ...     ...\n","49995  0.431281  0.891558       0\n","49996  0.391986  0.877517       0\n","49997  0.417829  0.916618       0\n","49998  0.115487  0.916618       0\n","49999  0.163136  0.931397       0\n","\n","[50000 rows x 3 columns]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(f\"datasets/{dataset}/{dataset_name}.csv\")\n","df_task = df[df[\"task\"] == Task_Number]\n","df_task = df_task.drop('task', axis=1)\n","df_task"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for itter in range(1,iterations+1):\n","  Model = model_class(input_size=4,\n","        device=torch.device(\"cpu\"),\n","      \tnum_layers=num_layers,\n","        hidden_size=hidden_size,\n","        output_size=output_size,\n","        batch_size=batch_size,\n","          )\n","  Preprocess_object = Preprocess(seq_len=seq_len)\n","  with open(f\"Performance/Pretrain/{model_type}/{dataset}/Before/{dataset}-{model_type}-pretrain-hidden{hidden_size}-epoch10_itter{itter}.pickle\", \"wb\") as fp:\n","    pickle.dump(Model.state_dict(), fp)\n","\n","  if len(df_task) % batch_size == 0:\n","    n_batches = int(len(df_task) / batch_size)\n","  else:\n","    n_batches = int(len(df_task) / batch_size) + 1\n","  optimizer = torch.optim.Adam(Model.parameters(), lr=0.01)\n","  loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n","  out_h = None\n","  for j in range(0,10):\n","    for i in range(0, len(df_task), batch_size):\n","      x = df_task.iloc[i : i + batch_size, 0:-1].values.astype(np.float32)\n","      y = list(df_task.iloc[i : i + batch_size, -1])\n","      if len(y) >= seq_len:\n","        x = np.array(x)\n","        y = list(y)\n","        x, y, _ = Preprocess_object._load_batch(x, y)\n","        y_pred = Model(x)\n","        y_pred = get_samples_outputs(y_pred)\n","        pred, _ = get_pred_from_outputs(y_pred)\n","        loss = loss_fn(y_pred, y)\n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        # update weights\n","        optimizer.step()\n","  with open(f\"Performance/Pretrain/{model_type}/{dataset}/After/{dataset}-{model_type}-pretrain-hidden{hidden_size}-epoch10_itter{itter}.pickle\", \"wb\") as fp:\n","    pickle.dump(Model.state_dict(), fp)\n","  print(f'itter {itter} finished')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM82h1t6isZ5UsHdGaHl7jp","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"}},"nbformat":4,"nbformat_minor":0}
