{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('..')\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.pretrain import *\n",
    "from Models.cPB import cPB\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from Models.cGRU_Model import cGRU\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='torch.storage')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch.storage')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module='torch.storage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINE datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executable code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(1, 11):\n",
    "    ###### Config the parameters ###### \n",
    "    base_model = 'GRU'\n",
    "    batch_size = 1\n",
    "    hidden_size = 50\n",
    "    seq_len = 10\n",
    "    epoch_size = 10\n",
    "    lr = 0.01\n",
    "    number_of_tasks = 4\n",
    "    mask_selection_NofBatch = 50*128\n",
    "    input_size = 2\n",
    "    iteration = iter\n",
    "    dataset = 'SINE'\n",
    "    dataset_name = \"sine_rw10_mode5_extended_16-16_1234\"\n",
    "    Pretrain_task = '2'\n",
    "    output_size = 2\n",
    "    ####### End of Config ######\n",
    "    df = pd.read_csv(os.path.join(f\"datasets/{dataset}/\", f\"{dataset_name}.csv\"))\n",
    "    pretrain_model_addr = f'Performance/Pretrain/{base_model}/{dataset}/Before/SINE-Task_{Pretrain_task}-GRU-pretrain-hidden50-epoch10_iter1.pickle'\n",
    "\n",
    "    # Initialize cGRU model\n",
    "    model = cGRU(lr=lr, hidden_size=hidden_size, seq_len=seq_len, \n",
    "                 pretrain_model_addr=pretrain_model_addr, input_size=input_size, \n",
    "                 epoch_size=epoch_size, batch_first=True, output_size = output_size)\n",
    "\n",
    "    # loop for each task\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    prev_data_x = []\n",
    "    prev_data_y = []\n",
    "    \n",
    "    for task in range(1, df[\"task\"].max() + 1):\n",
    "      \n",
    "      first_batch = True\n",
    "      df_task = df[df[\"task\"] == task]\n",
    "      df_task = df_task.drop(columns=\"task\")\n",
    "      # loop based on each batch of data\n",
    "      batch_cont=0\n",
    "      x = []\n",
    "      y = []\n",
    "      for i in range(0, len(df_task), batch_size):\n",
    "\n",
    "        if len(x_batch)==128:\n",
    "          if first_batch==True:\n",
    "            first_batch = False\n",
    "          x_batch = []\n",
    "          y_batch = []\n",
    "          if first_batch == False:\n",
    "            prev_data_x = x[-seq_len:]\n",
    "            prev_data_y = y[-seq_len:]\n",
    "        x_batch.append(df_task.iloc[i, 0:-1].values.astype(np.float32))\n",
    "        y_batch.append(df_task.iloc[i, -1])\n",
    "        \n",
    "        x.append(df_task.iloc[i, 0:-1].values.astype(np.float32))\n",
    "        y.append(df_task.iloc[i, -1])\n",
    "        \n",
    "        if len(x) >= seq_len:\n",
    "          model.predict_one(x[-seq_len:],y[-seq_len:],task)\n",
    "          if len(x_batch)==128:\n",
    "            if first_batch:\n",
    "              model.learn_many_anytime(x_batch,y_batch)\n",
    "            else:\n",
    "              new_batch_x = np.concatenate([prev_data_x, x_batch], axis=0) \n",
    "              new_batch_y = np.concatenate([prev_data_y, y_batch], axis=0)           \n",
    "              model.learn_many_anytime(new_batch_x,new_batch_y)\n",
    "      all_y = df_task.iloc[:, -1].reset_index(drop=True)\n",
    "      model.save_final_metrics_anytime(task,all_y)\n",
    "\n",
    "    file_path = f'Performance/Results/cGRU/Anytime/{dataset}/pretrain_T{Pretrain_task}_{base_model}-{dataset_name}-hidden{hidden_size}-epoch10-iter{iteration}_periodic.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(model.performance_anytime, file)\n",
    "        \n",
    "    with open(f\"Performance/Final Models/cGRU/{base_model}/{dataset}/Anytime/{dataset_name}-{base_model}-hidden{hidden_size}-epoch10-iter{iteration}.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(model.model.state_dict(), fp)\n",
    "\n",
    "    print(f'Iteration {iteration} finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executable code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(1, 11):\n",
    "    ###### Config the parameters ######\n",
    "    base_model = 'GRU'\n",
    "    batch_size = 1\n",
    "    hidden_size = 250\n",
    "    seq_len = 11\n",
    "    epoch_size = 10\n",
    "    lr = 0.01\n",
    "    number_of_tasks = 4\n",
    "    mask_selection_NofBatch = 50*128\n",
    "    input_size = 4\n",
    "    iteration = iter\n",
    "    dataset = 'Weather'\n",
    "    dataset_name = \"weather_st124_1conf\"\n",
    "    Pretrain_task = '1'\n",
    "    output_size = 2\n",
    "    ###### End of Config the parameters ######\n",
    "    df = pd.read_csv(os.path.join(f\"datasets/{dataset}/\", f\"{dataset_name}.csv\"))\n",
    "    pretrain_model_addr = f'Performance/Pretrain/{base_model}/{dataset}/Before/{dataset}-{base_model}-pretrain-hidden{hidden_size}-epoch10_itter{iteration}.pickle'\n",
    "\n",
    "    # Initialize cGRU model\n",
    "    model = cGRU(lr=lr, hidden_size=hidden_size, seq_len=seq_len, \n",
    "                 pretrain_model_addr=pretrain_model_addr, input_size=input_size, \n",
    "                 epoch_size=epoch_size, batch_first=True, output_size = output_size)\n",
    "\n",
    "    # loop for each task\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    prev_data_x = []\n",
    "    prev_data_y = []\n",
    "    \n",
    "    for task in range(1, df[\"task\"].max() + 1):\n",
    "      \n",
    "      first_batch = True\n",
    "      df_task = df[df[\"task\"] == task]\n",
    "      df_task = df_task.drop(columns=\"task\")\n",
    "      # loop based on each batch of data\n",
    "      batch_cont=0\n",
    "      x = []\n",
    "      y = []\n",
    "      for i in range(0, len(df_task), batch_size):\n",
    "        if len(x_batch)==128:\n",
    "          if first_batch==True:\n",
    "            first_batch = False\n",
    "          x_batch = []\n",
    "          y_batch = []\n",
    "          if first_batch == False:\n",
    "            prev_data_x = x[-seq_len:]\n",
    "            prev_data_y = y[-seq_len:]\n",
    "\n",
    "        x_batch.append(df_task.iloc[i, 0:-1].values.astype(np.float32))\n",
    "        y_batch.append(df_task.iloc[i, -1])\n",
    "        \n",
    "        x.append(df_task.iloc[i, 0:-1].values.astype(np.float32))\n",
    "        y.append(df_task.iloc[i, -1])\n",
    "        \n",
    "        if len(x) >= seq_len:\n",
    "          model.predict_one(x[-seq_len:],y[-seq_len:],task)\n",
    "          if len(x_batch)==128:\n",
    "\n",
    "            if first_batch:\n",
    "              model.learn_many_anytime(x_batch,y_batch)\n",
    "            else:\n",
    "              new_batch_x = np.concatenate([prev_data_x, x_batch], axis=0) \n",
    "              new_batch_y = np.concatenate([prev_data_y, y_batch], axis=0)           \n",
    "              model.learn_many_anytime(new_batch_x,new_batch_y)\n",
    "      all_y = df_task.iloc[:, -1].reset_index(drop=True)\n",
    "      model.save_final_metrics_anytime(task,all_y)\n",
    "\n",
    "    file_path = f'Performance/Results/cGRU/Anytime/{dataset}/pretrain_T{Pretrain_task}_{base_model}-{dataset_name}-hidden{hidden_size}-epoch10-iter{iteration}_periodic.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(model.performance_anytime, file)\n",
    "        \n",
    "    with open(f\"Performance/Final Models/cGRU/{base_model}/{dataset}/Anytime/{dataset_name}-{base_model}-hidden{hidden_size}-epoch10-iter{iteration}.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(model.model.state_dict(), fp)\n",
    "\n",
    "\n",
    "    print(f'Iteration {iteration} finished')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM82h1t6isZ5UsHdGaHl7jp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CPB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
